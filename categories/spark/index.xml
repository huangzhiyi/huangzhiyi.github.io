<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>spark on Heis</title><link>https://huangzhiyi.github.io/categories/spark/</link><description>Recent content in spark on Heis</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sun, 24 May 2020 10:42:51 +0800</lastBuildDate><atom:link href="https://huangzhiyi.github.io/categories/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark 第八章 GraphFrames 图计算实验手册</title><link>https://huangzhiyi.github.io/spark-exp08/</link><pubDate>Sun, 24 May 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-exp08/</guid><description>«返回课程汇总页面
【实验手册版本】 当前版本号v20200524
版本 修改说明 v20200524 初始化版本
实验8.1 ：基于 GraphFrames 的 PageRank 网页排名 【实验名称】 实验8.1 ：基于 GraphFrames 的 PageRank 网页排名 【实验目的】 理解本地向量、转换器、评估器、参数
【实验原理】 调用 GraphFrame.pageRank 进行网页排名 【实验环境】 操作系统：Ubuntu 16.04 Spark：Spark2.x 开发环境：PyCharm 或 Jupyter Notebook graphframes:graphframes:0.8.0-spark2.4-s_2.11 【实验资源】 实验数据下载
https://pan.baidu.com/s/1JbaD95ObxiNXyz34KaP1Hg#提取码vkb5
【实验步骤】 确保虚拟机可以联网。
运行以下命令，安装 GraphFrames 库依赖。
spark-submit --packages graphframes:graphframes:0.8.0-spark2.4-s_2.11 hello.py
启用开发环境。 （1）如果使用 Jupyter Notebook 启动，需要加上 packages 选项和指定 graphframes 库版本。</description></item><item><title>Spark 第七章 Spark 机器学习库实验手册</title><link>https://huangzhiyi.github.io/spark-exp07/</link><pubDate>Sat, 09 May 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-exp07/</guid><description>«返回课程汇总页面
【实验手册版本】 当前版本号v20200509
版本 修改说明 v20200509 初始化版本
实验前准备 确保虚拟机连上网络，安装 Python 依赖库 numpy sudo pip3 install numpy
实验7.1：基本概念-本地向量、转换器、评估器、参数 【实验名称】 实验7.1：基本概念-本地向量、转换器、评估器、参数 【实验目的】 理解本地向量、转换器、评估器、参数
【实验原理】 （1）本地向量：Mllib支持两种类型的本地向量:密集向量(dense)和稀疏向量(sparse)。密集向量只有一个浮点数组组成，而一个稀疏向量必须有索引和一个浮点向量组成。例如，(2.1,3.2,4.3)代表一个密集向量。(3，[1.1,2.3],[5.6,4.3,4.4])代表一个稀疏向量。
（2）Transformer：翻译成转换器，是一种可以将一个DataFrame转换为另一个DataFrame的算法。比如一个模型就是一个 Transformer。它可以把一个不包含预测标签的测试数据集 DataFrame 打上标签，转化成另一个包含预测标签的 DataFrame。 技术上，Transformer实现了一个方法transform()，它通过附加一个或多个列将一个DataFrame转换为另一个DataFrame
（3）Estimator：翻译成估计器或评估器，它是学习算法或在训练数据上的训练方法的概念抽象。在 Pipeline 里通常是被用来操作 DataFrame 数据并生成一个 Transformer。从技术上讲，Estimator实现了一个方法fit()，它接受一个DataFrame并产生一个转换器。比如，一个随机森林算法就是一个 Estimator，它可以调用fit()，通过训练特征数据而得到一个随机森林模型。
（4）Parameter：所有Transformers和Estimators现在共享一个用于指定参数的通用API
【实验环境】 操作系统：Ubuntu 16.04 Spark：Spark2.x 开发环境：PyCharm或pyspark交互命令 【实验资源】 实验数据下载</description></item><item><title>Spark 第六章 Spark Streaming 实验手册</title><link>https://huangzhiyi.github.io/spark-exp06/</link><pubDate>Sun, 19 Apr 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-exp06/</guid><description>«返回课程汇总页面
【实验手册版本】 当前版本号v20200424
版本 修改说明 v20200424 新增修改日志级别的步骤 v20200419 初始化版本
实验6.1 ：运行网络版的WordCount 【实验名称】 实验6.1 ：运行网络版的WordCount 【实验目的】 了解NetCat工具的使用。 初步了解Streaming运行的方式。 【实验原理】 运行Spark自带的StreamingWordCount程序，
【实验环境】 操作系统：Ubuntu 16.04 （确保机器cpu核数大于接收器的数量，或Local模式线程数大于接收器数量） Spark：Spark2.x Pyspark 【实验步骤】 一、基于netcat的聊天室
启动 NetCat 服务端，并在1234端口监听 nc -lk 1234
使用xshell 打开一个新的选项卡，连接虚拟机。启动NetCat客户端，并连接Netcat服务端 nc localhost 1234
注意：如果客户端和服务端不在同一台机器，localhost 可以换成实际IP。
在服务端输入以下字符串，并按回车，可以在客户端收到消息，并打印出来。这里注意替换学号为你个人学号。 hello 你的学号
在客户端输入字符串，并按回车，可以在服务端收到消息，并打印出来。这里注意替换学号为你个人学号。 你好 你的学号
在 NetCat 客户端的选项卡使用ctrl+c终止客户端进程。</description></item><item><title>Spark 综合实验1</title><link>https://huangzhiyi.github.io/spark-training1/</link><pubDate>Sun, 12 Apr 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-training1/</guid><description>«返回课程汇总页面
【版本】 当前版本号v20200412
版本 修改说明 v20200412 初始化版本
Spark 综合实验1 【实验名称】 Spark 综合练习 【实验目的】 测试同学们对 Spark RDD 和 Spark SQL的掌握能力
【实验数据说明】 请从下面链接下载练习数据： https://pan.baidu.com/s/1Xj6s2evPcx8TpzpHkvjBDA#提取码u0jg people.csv 中的数据是中国第五次人口普查（2000年）和第六次人口普查（2010年）的数据。以下为数据列的说明
列序号 说明 1 地区名称 2 户口地区类型，分别为 城市/镇/乡村 3 户口集体类型，分别为 家庭户/集体户 4 统计年份 5 性别 6 人数
其中户口地区类型分为三种，即城市、镇和乡村。每个地区类型下，又按集体类型分为两种，家庭户和集体户。 【实验环境】 操作系统：Ubuntu 16.</description></item><item><title>Spark 课程资源汇总</title><link>https://huangzhiyi.github.io/spark-summary/</link><pubDate>Fri, 21 Feb 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-summary/</guid><description>实验报告模板下载 https://pan.baidu.com/s/1YGpTcPXnlht6Hb46YJpo-A#提取码mwum
实验报告要求 必须使用老师提供的实验报告模板。 实验步骤每一步都有详细的操作步骤说明，步骤顺序号，截图。 截图只截关键部分信息，不要全屏截图，如果截图信息太多需要标记截图关键信息。推荐使用 Snipaste（官网下载或网盘下载） 进行截图和标记。 实验手册 第二章实验手册
第三章实验手册
第四章实验手册
第五章实验手册
第六章实验手册
第七章实验手册
第八章实验手册
综合实验 综合实验1 课程课件资源 https://pan.baidu.com/s/1zSmrVAKFaccUc4cZXcQGxA#提取码vhwc
Spark Python API 文档 在线文档（官网英文）
RDD API 查询
DataFrame 查询
DStream API 查询
GraphFrame API 查询
作业提交 除了特殊原因申请延后提交实验报告的同学，其他同学请按照时间提交实验报告</description></item><item><title>Spark 第三章使用 Python 开发 Spark 应用实验手册</title><link>https://huangzhiyi.github.io/spark-exp03/</link><pubDate>Fri, 14 Feb 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-exp03/</guid><description>«返回课程汇总页面
【实验手册版本】 当前版本号v20200316
版本 修改说明 v20200316 修改实验3.3中访问 Jupytor Notebook 注意要点。 v20200226 新增选做实验 v20200214 初始化版本
实验3.1：PySpark 命令行的应用 【实验名称】 实验3.1：PySpark 的应用
【实验目的】 掌握PySpark 的应用 【实验原理】 pyspark -h 查看用法 Usage: pyspark [options]
常见的[options] 如下表
【实验环境】 Ubuntu 16.04 Python 3 PySpark spark 2.4.4 scala 2.12.10 【实验步骤】 1、输入pyspark -h查看各参数的定义
pyspark -h
2、查看sc变量 （1）不指定 master 时</description></item><item><title>Spark 第五章 SparkSQL 实验手册</title><link>https://huangzhiyi.github.io/spark-exp05/</link><pubDate>Fri, 14 Feb 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-exp05/</guid><description>«返回课程汇总页面
【实验手册版本】 当前版本号v20200418
版本 修改说明 v20200418 修改 hosts 的说明，增加常见问题解答 v20200408 加入hosts的检查 v20200407 修正实验5.3一处描述错误 v20200406 修复实验5.3复制命令为cp v20200322 初始化版本
实验5.1：RDD DataFrame API 练习 【实验名称】 RDD DataFrame API 练习 【实验目的】 掌握RDD DataFrame 的相关 API
【实验原理】 DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维 表 格。 DataFrame 与 RDD 的 主 要区 别在 于， 前 者带 有 schema 元信 息 ，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。采用RDD的toDF()算子将RDD转为DataFrame。</description></item><item><title>Spark 第四章 Spark RDD 实验手册</title><link>https://huangzhiyi.github.io/spark-exp04/</link><pubDate>Fri, 14 Feb 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-exp04/</guid><description>«返回课程汇总页面
【实验手册版本】 当前版本号v20200317
版本 修改说明 v20200317 实验4.2，修正练习（8），应该是求交集 v20200310 初始化版本
实验4.1：RDD 的创建 【实验名称】 实验4.1：RDD 的创建
【实验目的】 掌握RDD的创建的方式 【实验原理】 略
【实验环境】 Ubuntu 16.04 Python 3 PySpark spark 2.4.4 Hadoop 2.7.3 【实验步骤】 在/home/hadoop路径下创建一个文本文件，并命名为123.txt（注意替换123为你学号的后三位）。并输入以下内容，注意替换其中汉字为你个人学号后三位。 你学号后三位,997,953,932,877,453
启动 hadoop，并把第一步创建的文本文件上传到 HDFS。 （1）启动 Hadoop
start-hdp.sh
（2）上传文本文件
hdfs dfs -mkdir -p /spark-exp4
hdfs dfs -put /home/hadoop/123.txt /spark-exp4/
（3）查看是否上传成功</description></item><item><title>Spark 第二章搭建 Spark 实验手册</title><link>https://huangzhiyi.github.io/spark-exp02/</link><pubDate>Mon, 10 Feb 2020 10:42:51 +0800</pubDate><guid>https://huangzhiyi.github.io/spark-exp02/</guid><description>«返回课程汇总页面
实验2.1：搭建 Spark Standalone 模式 【版本】 当前版本号v20200318
版本 修改说明 v20200318 修改了第16步脚本路径 v20200229 增加了hosts的设置 v20200210 初始化版本
### 【实验名称】
实验2.1：搭建 Spark Standalone 模式 【实验目的】 掌握搭建 Spark Standalone 模式 熟练掌握Linux命令（vi、tar、环境变量修改等等）的使用 掌握VMWare、XShell、Xftp等客户端的使用 【实验原理】 Spark支持多种分布式部署方式，至少包括：
Standalone单独部署（伪分布或全分布），不需要有依赖资源管理器。 Hadoop YARN（也即Spark On Yarn），部署到Hadoop YARN资源管理器。 Apache Mesos，部署到Apache Mesos资源管理器。 Amazon EC2，部署到Amazon EC2资源管理器。 这里主要学习单独（Standalone）部署中的伪分布模式的搭建。
【实验环境】 内存：至少4G 硬盘：至少空余40G 操作系统: 64位 Windows系统。 【实验资源】 以下非网盘实验资源推荐复制链接到迅雷下载。</description></item></channel></rss>
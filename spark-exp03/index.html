<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=zh-CN lang=zh-cn><head><link href=//gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.80.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>Spark 第三章使用 Python 开发 Spark 应用实验手册 &#183; Heis</title><link type=text/css rel=stylesheet href=https://huangzhiyi.github.io/css/heis-min.css?201030><link type=text/css id=dark_theme_css rel=stylesheet href=https://huangzhiyi.github.io/css/a11y-dark.css title=dark><link type=text/css id=light_theme_css rel="alternate stylesheet" href=https://huangzhiyi.github.io/css/a11y-light.css title=light><script>var _hmt=_hmt||[];(function(){var hm=document.createElement("script");hm.src="https://hm.baidu.com/hm.js?f58b6a2675cf52000232edb4d109eccc";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed2.png><link rel="shortcut icon" href=/favicon2.png><script src=https://huangzhiyi.github.io/js/jquery-min.js></script></head><body><aside class=sidebar id=bsidebar><button id=hide-sb-btn onclick=hideSidebar()></button><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://huangzhiyi.github.io/><h1 id=side-title>Heis</h1></a><p class=lead>Long live MAMBA</p><p class=ac><a href=https://support.qq.com/products/332688 class=tooltip target=_blank><img class=tooltiptext src=https://huangzhiyi.github.io/feedback.png?20210617>
提点意见和建议»</a></p></div><nav><ul class=sidebar-nav><li><a href=https://huangzhiyi.github.io/>首页</a></li><li><a href=http://go.heisun.xyz/ target=_blank>课程导航»</a></li><li><a href=https://huangzhiyi.github.io/categories/javaweb/>Java后台应用项目开发»</a></li><li><a href=https://huangzhiyi.github.io/categories/clouddc/>云数据中心基础»</a></li><li><a href=https://huangzhiyi.github.io/categories>文章分类»</a></li><li><a href=https://huangzhiyi.github.io/tags>文章标签»</a></li><li><a href=https://heis.gitee.io/ target=_blank>镜像0：heis.gitee.io</a></li><li><a href=http://heisun.xyz/ target=_blank>镜像1：heisun.xyz</a></li><li><a href=https://huangzhiyi.github.io/ target=_blank>镜像2：huangzhiyi.github.io</a></li></ul></nav><p>&copy; 2021. All rights reserved.</p><p></p></div></aside><main class="content container" id=mainpanel><button id=show-sb-btn onclick=showSidebar()></button><div class=post><h1>Spark 第三章使用 Python 开发 Spark 应用实验手册</h1><time datetime=2020-02-14T10:42:51+0800 class=post-date>2020-02-14</time><blockquote><h2>文章导航</h2><aside><nav id=TableOfContents><ul><li><a href=#实验手册版本>【实验手册版本】</a></li><li><a href=#实验31pyspark-命令行的应用>实验3.1：PySpark 命令行的应用</a><ul><li><a href=#实验名称>【实验名称】</a></li><li><a href=#实验目的>【实验目的】</a></li><li><a href=#实验原理>【实验原理】</a></li><li><a href=#实验环境>【实验环境】</a></li><li><a href=#实验步骤>【实验步骤】</a></li></ul></li><li><a href=#实验32搭建-pycharm-远程开发-spark-应用>实验3.2：搭建 PyCharm 远程开发 Spark 应用</a><ul><li><a href=#实验名称-1>【实验名称】</a></li><li><a href=#实验目的-1>【实验目的】</a></li><li><a href=#实验原理-1>【实验原理】</a></li><li><a href=#实验资源>【实验资源】</a></li><li><a href=#实验环境-1>【实验环境】</a></li><li><a href=#实验步骤-1>【实验步骤】</a></li></ul></li><li><a href=#实验33搭建-jupyter-notebook选做>实验3.3：搭建 Jupyter Notebook（选做）</a><ul><li><a href=#实验名称-2>【实验名称】</a></li><li><a href=#实验目的-2>【实验目的】</a></li><li><a href=#实验原理-2>【实验原理】</a></li><li><a href=#实验环境-2>【实验环境】</a></li><li><a href=#实验步骤-2>【实验步骤】</a></li></ul></li></ul></nav></aside></blockquote><p><a href=/spark-summary/>«返回课程汇总页面</a></p><h2 id=实验手册版本>【实验手册版本】</h2><p>当前版本号<code>v20200316</code></p><div class=tbl-start></div><table><thead><tr><th>版本</th><th>修改说明</th></tr></thead><tbody><tr><td>v20200316</td><td>修改实验3.3中访问 Jupytor Notebook 注意要点。</td></tr><tr><td>v20200226</td><td>新增选做实验</td></tr><tr><td>v20200214</td><td>初始化版本</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h2 id=实验31pyspark-命令行的应用>实验3.1：PySpark 命令行的应用</h2><h3 id=实验名称>【实验名称】</h3><p>实验3.1：PySpark 的应用</p><h3 id=实验目的>【实验目的】</h3><ul><li>掌握PySpark 的应用</li></ul><h3 id=实验原理>【实验原理】</h3><ul><li>pyspark -h 查看用法</li></ul><pre><code>Usage: pyspark [options]
常见的[options] 如下表
</code></pre><p><img src=/static/img/pyspark-usege.png alt></p><h3 id=实验环境>【实验环境】</h3><ul><li>Ubuntu 16.04</li><li>Python 3</li><li>PySpark</li><li>spark 2.4.4</li><li>scala 2.12.10</li></ul><h3 id=实验步骤>【实验步骤】</h3><p>1、输入<code>pyspark -h</code>查看各参数的定义</p><pre><code>pyspark -h
</code></pre><p>2、查看sc变量
（1）不指定 master 时</p><pre><code>pyspark
</code></pre><p>查看sc变量</p><pre><code>sc
</code></pre><p>退出PySpark</p><pre><code>quit()
</code></pre><p>（2）指定 master 时
启动 Spark</p><pre><code>start-spark.sh
</code></pre><pre><code>pyspark --master spark://node0:7077
</code></pre><p>查看sc变量</p><pre><code>sc
</code></pre><p>3、使用pyspark开发一个WordCount程序。
（1）在本地创建一个文件：<code>/home/hadoop/你的学号/wc.txt</code> (001表示学号，请根据情况修改)。此路径也可以换为hdfs的路径。</p><p>wc.txt文件中的内容是：</p><pre><code>Hi 你的名字
count the chicks and rabbits in the cage
chick rabbit chick rabbit chick rabbit
rabbit rabbit chick rabbit chick
chick rabbit chick rabbit chick rabbit
</code></pre><p>（2）多行代码实现WordCount。</p><pre><code>#读取文件
r1=sc.textFile(&quot;/home/hadoop/你的学号/wc.txt&quot;)
r1.collect()

#分行
r2=r1.flatMap(lambda line:line.split(&quot; &quot;))
r2.collect()

#统计单词数量
r3=r2.map(lambda w:(w,1))
r3.collect()

#单词次数累加
r4=r3.reduceByKey(lambda x,y: x+y)
r4.collect()

#按次数排序
r5=r4.sortBy(lambda x:x[1],False)
r5.collect()
</code></pre><blockquote><p>注:关于flatMap,map,reduceByKey等可参考书本第四章的算子部分。</p></blockquote><p>（3）一行代码实现 WordCount，并输出到文件。</p><pre><code>sc.textFile(&quot;/home/hadoop/你的学号/wc.txt&quot;).flatMap(lambda line:line.split(&quot; &quot;)).map(lambda w:(w,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x:x[1],False).saveAsTextFile(&quot;/home/hadoop/你的学号/wc-result&quot;)
</code></pre><blockquote><p>提示:运行此命令每次需要更改saveAsTextFile后面的文件输出路径，否则会出错提示"FileAlreadyExistException"</p></blockquote><p>（4）退出PySpark，访问<code>/home/hadoop/你的学号/wc-result</code>目录，并查看输出目录下的文件。</p><p>（5）查看结果可能由多个文件（分区）组成。尝试重复步骤（3），修改代码使用repartition方法，把结果输出到一个分区。</p><h2 id=实验32搭建-pycharm-远程开发-spark-应用>实验3.2：搭建 PyCharm 远程开发 Spark 应用</h2><h3 id=实验名称-1>【实验名称】</h3><p>实验3.2：搭建 PyCharm 远程开发 Spark 应用</p><h3 id=实验目的-1>【实验目的】</h3><ul><li>掌握 PyCharm 的使用</li><li>掌握 PyCharm 远程开发 Spark 应用</li></ul><h3 id=实验原理-1>【实验原理】</h3><ul><li>通过 PyCharm 的设置,可以把本地电脑文件通过 SFTP 发送到虚拟机运行，从而能够实现远程的开发。</li></ul><h3 id=实验资源>【实验资源】</h3><ul><li>pycharm-professional-2019.3.3.exe</li></ul><pre><code>https://download-cf.jetbrains.com/python/pycharm-professional-2019.3.3.exe
</code></pre><ul><li>pycharm 安装指南</li></ul><pre><code>https://pan.baidu.com/s/1-9Lu6beSXCj2RYp9hwpn5A#提取码=fjji
</code></pre><ul><li>Python环境 Anaconda（windows平台）</li></ul><pre><code>https://repo.continuum.io/archive/Anaconda3-5.3.1-Windows-x86_64.exe
</code></pre><h3 id=实验环境-1>【实验环境】</h3><ul><li>Ubuntu 16.04</li><li>Python 3</li><li>PySpark</li><li>spark 2.4.4</li><li>scala 2.12.10</li></ul><h3 id=实验步骤-1>【实验步骤】</h3><ol><li><p>在本机 Windows 安装 Anaconda。</p></li><li><p>安装 PyCharm professional。</p></li><li><p>查看 pycharm 安装指南进行配置。</p></li><li><p>安装完成后，打开PyCharm，新建一个名字叫<code>spark-exp</code>的项目，项目路径可以自己定义，这里以<code>D:\workspaces\workspace_python\spark-exp</code>为例。
<img src=/static/img/chp03-createworkspace.png alt></p></li><li><p>打开菜单"Tools -> Deployment -> Configuration&mldr;"
<img src=/static/img/chp03-new-deployment.png alt></p></li><li><p>这里需要新建一个通过 SFTP 把本地文件远程发布到虚拟机的设置。
<img src=/static/img/chp03-new-sftp.png alt></p></li><li><p>输入名称<code>hadoop@node0</code>
<img src=/static/img/chp03-new-sftp-server.png alt></p></li><li><p>输入虚拟机的地址<code>192.168.30.130</code>，用户名<code>hadoop</code>，密码<code>Hdp0668</code>。
<img src=/static/img/chp03-new-sftp-server-settings.png alt></p></li></ol><p>测试连接成功后，保存并退出。
<img src=/static/img/chp03-test-connection.png alt></p><ol start=9><li><p>打开菜单"File -> Settings"
<img src=/static/img/chp03-open-settings.png alt></p></li><li><p>新增一个 Interpreter（Python解析器），这里我们需要设置虚拟机的 Python 解析器相关设定，这样我们通过 SFTP 从本地发送到虚拟机的Python 脚本才能知道使用哪个解析器进行解析运行。
<img src=/static/img/chp03-add-interpreter.png alt>
<img src=/static/img/chp03-add-interpreter-settings.png alt></p></li><li><p>选择"ssh-interpreter"，配置虚拟机的地址，用户名和密码。
<img src=/static/img/chp03-new-ssh-interpreter.png alt>
<img src=/static/img/chp03-new-ssh-interpreter-pwd.png alt></p></li><li><p>interpreter 需要选择Python的所在路径，这里设置为<code>/usr/bin/python3</code>。把本地项目路径<code>D:/workspaces/workspace_python/spark-exp</code>映射到虚拟机的路径<code>/home/hadoop/spark-exp</code>，如果虚拟机路径不存在请先创建。完成以后点击"Finish"。</p></li></ol><p><img src=/static/img/chp03-new-ssh-interpreter-path.png alt></p><ol start=13><li><p>在 spark-exp 项目下新建一个 wordcount2.py 文件。
<img src=/static/img/chp03-new-python-wc.png alt></p></li><li><p>wordcount2 输入以下代码，注意修改你的学号。</p></li></ol><pre><code>from pyspark import SparkContext
sc = SparkContext(&quot;spark://node0:7077&quot;, &quot;WordCountApp&quot;)
rs = sc.textFile(&quot;/home/hadoop/你的学号/wc.txt&quot;).flatMap(lambda line: line.split(&quot; &quot;)).map(lambda w: (w, 1)).reduceByKey(lambda x, y: x+y).sortBy(lambda x:x[1], False).collect()
for e in rs:
    print(e)

</code></pre><ol start=15><li><p>把wordcount2.py 文件上传到虚拟机。
<img src=/static/img/chp03-deployment-wc2.png alt></p></li><li><p>编辑 python 脚本的运行设置模板。
<img src=/static/img/chp03-edit-configuration.png alt></p></li><li><p>新增一个 python 运行设置模板。
<img src=/static/img/chp03-new-python-run-conf.png alt></p></li><li><p>在环境变量中增加以下环境变量</p></li></ol><pre><code>SPARK_HOME    /opt/spark
PYTHONPATH    /opt/spark/python
JAVA_HOME     /opt/jdk8
HADOOP_HOME   /opt/hadoop
SCALA_HOME    /opt/scala2-12
</code></pre><p><img src=/static/img/chp03-new-env.png alt></p><ol start=19><li><p>把"Script path"配置设置为本地 wordcount2.py 文件路径。&ldquo;Python interpreter&rdquo; 选择刚创建的解析器。
<img src=/static/img/chp03-new-run-conf1.png alt></p></li><li><p>从模板创建一个运行设置
<img src=/static/img/chp03-new-run-templ.png alt></p></li><li><p>启动虚拟机的spark</p></li></ol><pre><code>start-spark.sh
</code></pre><ol start=22><li>运行 wordcount2 看是否能够得到结果。</li></ol><p><img src=/static/img/chp03-run-wc2.png alt></p><h2 id=实验33搭建-jupyter-notebook选做>实验3.3：搭建 Jupyter Notebook（选做）</h2><h3 id=实验名称-2>【实验名称】</h3><p>实验3.3：搭建 Jupyter Notebook（选做）</p><h3 id=实验目的-2>【实验目的】</h3><ul><li>搭建 Jupyter Notebook 开发环境</li></ul><h3 id=实验原理-2>【实验原理】</h3><p>Jupyter Notebook 是一个基于Web 的交互编程环境，支持多种编程语言。使用他来替代命令行交互编程可以获得更好的编程体验。</p><h3 id=实验环境-2>【实验环境】</h3><ul><li>Ubuntu 16.04</li><li>Python 3.5</li><li>PySpark</li><li>spark 2.4.4</li><li>scala 2.12.10</li></ul><h3 id=实验步骤-2>【实验步骤】</h3><ol><li>配置 Python 依赖包的源，Python使用pip 来下载依赖的包。但是原有的下载源下载资源太慢，这里我们改用清华大学的安装源。</li></ol><pre><code>mkdir ~/.pip/
vim ~/.pip/pip.conf
</code></pre><ol start=2><li>在 pip.conf 文件里面输入以下内容，修改源为清华大学源。</li></ol><pre><code>[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host = pypi.tuna.tsinghua.edu.cn
</code></pre><ol start=3><li>通过网络下载安装 Jupyter Notebook 前，先检查一下能否连接互联网，如果命令没有响应，请返回实验2检查网络安装和配置。</li></ol><pre><code>#测试能否联网
ping baidu.com
</code></pre><ol start=4><li>安装 Jupyter Notebook。</li></ol><pre><code>pip3 install notebook
</code></pre><ol start=5><li>因为安装 ipython 版本和 Python3.5 不兼容，所以卸载自带 ipython，安装7.9.0版本。</li></ol><pre><code>pip3 uninstall ipython
pip3 install ipython==7.9.0
</code></pre><ol start=6><li>安装 findspark，主要用来检测spark运行。</li></ol><pre><code>pip3 install findspark
</code></pre><ol start=7><li>修改 PySpark 的驱动命令，修改为使用 Jupyter Notebook。</li></ol><pre><code>vim /opt/spark/conf/spark-env.sh
</code></pre><p>找到这句，注释掉。</p><pre><code>#export PYSPARK_DRIVER_PYTHON=python3
</code></pre><ol start=8><li>在用户的环境变量配置文件增加 PYSPARK 驱动配置。
（1）用户的环境变量配置</li></ol><pre><code>vim ~/.bashrc
</code></pre><p>（2）在文件最后加入</p><pre><code>#由于上文注释掉此选项以后，pyspark 将不能在SSH终端（例如Xshell）输入 Python 代码，而是自动启动 Jupyter Notebook，用户可以在 Notebook 输入代码。
export PYSPARK_DRIVER_PYTHON=jupyter
#--ip=* 选项主要是为了启动 Jupyter 服务器可以绑定所有IP，浏览器可以访问。
export PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=*'
</code></pre><p>（3）让配置生效</p><pre><code>source ~/.bashrc
</code></pre><ol start=9><li>启动PySpark，查看是否能够启动 Jupyter Notebook。</li></ol><pre><code>pyspark
</code></pre><p>复制命令行下 Jupyter Notebook 的地址。
<img src=/static/img/jupyter-token.png alt></p><blockquote><p>注意：</p><ol><li>Jupyter Notebook每次地址token参数会不一样。</li><li>如果 Jupyter Notebook 地址不能访问可以替换 node0 为虚拟机 IP 地址，或者编辑 Windows 的 host 文件（C:\Windows\System32\drivers\etc\hosts），增加一行"192.168.30.130 node0"，把node0指向虚拟机地址，即可访问。</li></ol></blockquote><ol start=10><li>访问 Jupyter Notebook 的地址，新建一个文件，Jupyter 会自动在你启动的目录下生成一个<code>Untitled.ipynb</code>的文件来保存你输入的代码。</li></ol><p><img src=/static/img/jupyter-new.png alt></p><p><img src=/static/img/jupyter-run.png alt></p><blockquote><p>提示：</p><ol><li>在命令行按下<code>Ctrl+C</code>可以停止 Jupyter Notebook。</li><li>如果安装了 Jupyter Notebook，原本 PySpark 的命令行交互方式就不能再使用了。如果想要恢复，就把第7步注释的部分还原，把第8步的环境变量配置注释掉。</li></ol></blockquote><ol start=11><li><p>自行探索 Jupyter Notebook 的保存，编辑等功能。</p></li><li><p>重复执行试验3.2 的 wordcount 部分代码，检验 Jupyter Notebook 是否正常运行。</p></li></ol></div><button id=scroll-top-btn>↑顶部</button>
<button id=switch-theme-btn>换主题</button><div id=outerdiv style=position:fixed;top:0;left:0;background:rgba(0,0,0,.7);z-index:2;width:100%;height:100%;display:none><div id=innerdiv style=position:absolute><img id=bigimg style="border:5px solid #fff" src></div></div><div id=category style=display:none;position:fixed;right:10px;bottom:10px;width:10em></div><script src=https://huangzhiyi.github.io/js/highlight.pack.js></script><script src=https://huangzhiyi.github.io/js/highlightjs-line-numbers.min.js></script><script>hljs.initHighlightingOnLoad();hljs.initLineNumbersOnLoad();$(function(){let curTheme=getCookie("theme");if(curTheme===""){setTheme("dark");}else{setTheme(curTheme);}
$('div.tbl-start').nextUntil('div.tbl-end','table').addClass('tbl');$("img").click(function(){var _this=$(this);imgShow("#outerdiv","#innerdiv","#bigimg",_this);});$("#scroll-top-btn").click(function(){document.body.scrollTop=document.documentElement.scrollTop=0;});$("#switch-theme-btn").click(function(){switchTheme();})});</script><script src=https://huangzhiyi.github.io/js/clipboard.min.js?></script><script src=https://huangzhiyi.github.io/js/codecopy.bundle.js?857></script></main><script src=https://huangzhiyi.github.io/js/heis-custom-min.js?938></script></body></html>
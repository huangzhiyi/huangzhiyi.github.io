<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=zh-CN lang=zh-cn><head><link href=//gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.80.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>Hadoop Part 10 - Flume 和 Sqoop 操作实例（选做） &#183; Heis</title><link type=text/css rel=stylesheet href=https://huangzhiyi.github.io/css/heis-min.css?20210716><script>var _hmt=_hmt||[];(function(){var hm=document.createElement("script");hm.src="https://hm.baidu.com/hm.js?f58b6a2675cf52000232edb4d109eccc";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed2.png><link rel="shortcut icon" href=/favicon2.png></head><body><style>.tooltip{position:relative;display:inline-block;border-bottom:1px dotted #000}.tooltip .tooltiptext{visibility:hidden;width:auto;max-width:200px;background-color:#000;color:#fff;text-align:center;position:absolute;z-index:1}.tooltip:hover .tooltiptext{visibility:visible;position:absolute;top:2rem;left:0;z-index:1000}</style><aside class=sidebar id=bsidebar><button id=hide-sb-btn onclick=hideSidebar()></button><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://huangzhiyi.github.io/><h1 id=side-title>Heis</h1></a><p class=lead>Long live MAMBA</p><p class=ac><a href=https://support.qq.com/products/332688 class=tooltip target=_blank><img class=tooltiptext src=https://huangzhiyi.github.io/feedback.png?20210617>
提点意见和建议»</a></p></div><nav><ul class=sidebar-nav><li><a href=https://huangzhiyi.github.io/>首页</a></li><li><a href=http://go.heisun.xyz/ target=_blank>课程导航»</a></li><li><a href=https://huangzhiyi.github.io/categories/javaweb/>Java后台应用项目开发»</a></li><li><a href=https://huangzhiyi.github.io/categories/clouddc/>云数据中心基础»</a></li><li><a href=https://huangzhiyi.github.io/categories>文章分类»</a></li><li><a href=https://huangzhiyi.github.io/tags>文章标签»</a></li><li><a href=https://heis.gitee.io/ target=_blank>镜像0：heis.gitee.io</a></li><li><a href=http://heisun.xyz/ target=_blank>镜像1：heisun.xyz</a></li><li><a href=https://huangzhiyi.github.io/ target=_blank>镜像2：huangzhiyi.github.io</a></li></ul></nav><p>&copy; 2021. All rights reserved.</p><p></p></div></aside><main class="content container" id=mainpanel><button id=show-sb-btn onclick=showSidebar()></button>
<script src=https://huangzhiyi.github.io/js/jquery-min.js></script><div class=post><h1>Hadoop Part 10 - Flume 和 Sqoop 操作实例（选做）</h1><time datetime=2021-06-01T10:20:00+0800 class=post-date>2021-06-01</time><blockquote><h2>文章导航</h2><aside><nav id=TableOfContents><ul><li><ul><li><a href=#版本>【版本】</a></li></ul></li><li><a href=#实验101----部署-flume>实验10.1 - 部署 Flume</a><ul><li><a href=#实验名称>【实验名称】</a></li><li><a href=#实验目的>【实验目的】</a></li><li><a href=#实验环境>【实验环境】</a></li><li><a href=#实验资源>【实验资源】</a></li><li><a href=#实验步骤>【实验步骤】</a></li></ul></li><li><a href=#实验102-flume-的配置与使用-avrosource---memorychannel---loggersink>实验10.2 Flume 的配置与使用 Avro(Source) -> Memory(Channel) -> Logger(Sink)</a><ul><li><a href=#实验名称-1>【实验名称】</a></li><li><a href=#实验目的-1>【实验目的】</a></li><li><a href=#实验原理>【实验原理】</a></li><li><a href=#实验环境-1>【实验环境】</a></li><li><a href=#实验资源-1>【实验资源】</a></li><li><a href=#实验步骤-1>【实验步骤】</a></li></ul></li><li><a href=#实验-103-flume-的配置与使用-syslogsource---memorychannel---hdfssink>实验 10.3 Flume 的配置与使用 Syslog(Source) -> Memory(Channel) -> HDFS(Sink)</a><ul><li><a href=#实验名称flume-的配置与使用-2>【实验名称】Flume 的配置与使用 2</a></li><li><a href=#实验目的-2>【实验目的】</a></li><li><a href=#实验原理-1>【实验原理】</a></li><li><a href=#实验环境-2>【实验环境】</a></li><li><a href=#实验资源-2>【实验资源】</a></li><li><a href=#实验步骤-2>【实验步骤】</a></li></ul></li><li><a href=#实验-104-sqoop-的安装>实验 10.4 Sqoop 的安装</a><ul><li><a href=#实验名称实验-104-sqoop-的安装>【实验名称】实验 10.4 Sqoop 的安装</a></li><li><a href=#实验目的-3>【实验目的】</a></li><li><a href=#实验环境-3>【实验环境】</a></li><li><a href=#实验资源-3>【实验资源】</a></li><li><a href=#实验步骤-3>【实验步骤】</a></li></ul></li><li><a href=#实验-105-使用-sqoop-进行数据转换>实验 10.5 使用 Sqoop 进行数据转换</a><ul><li><a href=#实验名称105-使用-sqoop-进行数据转换>【实验名称】10.5 使用 Sqoop 进行数据转换</a></li><li><a href=#实验目的-4>【实验目的】</a></li><li><a href=#实验环境-4>【实验环境】</a></li><li><a href=#实验资源-4>【实验资源】</a></li><li><a href=#实验步骤-4>【实验步骤】</a></li><li><a href=#实验步骤--导入数据从-mariadb-到-hdfs>【实验步骤- 导入数据从 MariaDB 到 HDFS】</a></li><li><a href=#实验步骤--导入数据从-mariadb-到-hive>【实验步骤- 导入数据从 MariaDB 到 Hive】</a></li><li><a href=#实验步骤--导入数据从-mariadb-到-hbase>【实验步骤- 导入数据从 MariaDB 到 HBase】</a></li><li><a href=#实验步骤--导出数据从-hdfs-到-mariadb>【实验步骤- 导出数据从 HDFS 到 MariaDB】</a></li></ul></li></ul></nav></aside></blockquote><p><a href=/hadoop-c-summary/>«返回课程汇总页面</a></p><h3 id=版本>【版本】</h3><p>当前版本号<code>v20210611</code></p><div class=tbl-start></div><table><thead><tr><th>版本</th><th>修改说明</th></tr></thead><tbody><tr><td>v20210611</td><td>新增Sqoop的内容</td></tr><tr><td>v20210607</td><td>初始化版本</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h2 id=实验101----部署-flume>实验10.1 - 部署 Flume</h2><h3 id=实验名称>【实验名称】</h3><p>实验10.1 - 部署 Flume</p><h3 id=实验目的>【实验目的】</h3><ul><li>部署 Flume</li></ul><h3 id=实验环境>【实验环境】</h3><ul><li>Windows 7 以上64位操作系统</li><li>JDK8</li><li>VMWare Workstation Pro</li><li>Hadoop 2.7.3</li><li>CentOS 7</li></ul><h3 id=实验资源>【实验资源】</h3><pre><code>下载链接：https://pan.baidu.com/s/1ghde86wcK6pwg1fdSSWg0w 
提取码：v3wv 
</code></pre><h3 id=实验步骤>【实验步骤】</h3><ol><li>在NodeA节点运行以下语句，注意提升为root权限执行。</li></ol><ul><li>创建 Flume 的安装目录。</li></ul><pre><code>su
mkdir /opt/flume
chown hadoop:wheel /opt/flume
</code></pre><ul><li>提升 root 用户权限执行以下语句，加入 ZooKeeper 环境变量。</li></ul><pre><code>su
echo &quot;export FLUME_HOME=/opt/flume
export PATH=\$FLUME_HOME/bin:\$PATH&quot; &gt;&gt;/etc/profile
</code></pre><ul><li>切换回 hadoop 用户</li></ul><pre><code>su hadoop
</code></pre><ul><li>使环境变量生效。</li></ul><pre><code>source /etc/profile
</code></pre><ol start=2><li>使用 hadoop 登录NodeA节点。</li></ol><pre><code>su hadoop
</code></pre><ol start=3><li><p>上传 Flume 安装包<code>apache-flume-1.8.0-bin.tar.gz</code> 到 NodeA 节点 <code>/home/hadoop</code> 目录。</p></li><li><p>解压<code>apache-flume-1.8.0-bin.tar.gz</code>到<code>/home/hadoop</code> 目录。</p></li></ol><pre><code>tar -xvf apache-flume-1.8.0-bin.tar.gz 
</code></pre><ol start=5><li>把解压以后的目录移到安装目录</li></ol><pre><code>sudo mv ~/apache-flume-1.8.0-bin/* /opt/flume
</code></pre><ol start=6><li>输入命令查看 Flume 版本</li></ol><pre><code>flume-ng version
</code></pre><h2 id=实验102-flume-的配置与使用-avrosource---memorychannel---loggersink>实验10.2 Flume 的配置与使用 Avro(Source) -> Memory(Channel) -> Logger(Sink)</h2><h3 id=实验名称-1>【实验名称】</h3><p>实验10.2 Flume 的配置与使用 Avro Source + Memory Channel + Logger Sink</p><h3 id=实验目的-1>【实验目的】</h3><ul><li><ol><li>理解 Flume 的基本原理，掌握各组件的作用及关系。</li></ol></li><li><ol start=2><li>熟悉 Flume 的常用配置。</li></ol></li><li></li></ul><h3 id=实验原理>【实验原理】</h3><p>Avro Source 会启动一个 RPC 的 Netty 服务器，此实验配置监听端口为 4141 。Avro Source 通过监听发送过来的文件，通过 Flume 的 Channel，输出到 Logger Sink。Logger Sink 可以打印文件的内容到控制台。</p><h3 id=实验环境-1>【实验环境】</h3><ul><li>Windows 7 以上64位操作系统</li><li>JDK8</li><li>VMWare Workstation Pro</li><li>Hadoop 2.7.3</li><li>CentOS 7</li><li>Flume 1.8.0 （下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz）</li></ul><h3 id=实验资源-1>【实验资源】</h3><pre><code>下载链接：https://pan.baidu.com/s/1ghde86wcK6pwg1fdSSWg0w 
提取码：v3wv 
</code></pre><h3 id=实验步骤-1>【实验步骤】</h3><ol><li>创建 Agent 配置文件/opt/flume/conf/avro.conf，并按如下配置。</li></ol><pre><code>vim /opt/flume/conf/avro.conf
</code></pre><pre><code>agent1.sources = srch2
agent1.sinks = ssink2
agent1.channels = ch1

# 配置 Source 监听端口为4141的 avro 服务
agent1.sources.srch2.type = avro
agent1.sources.srch2.bind = 0.0.0.0
agent1.sources.srch2.port = 4141
agent1.sources.srch2.channels = ch1

# 配置 Sink
agent1.sinks.ssink2.type = logger
agent1.sinks.ssink2.channel = ch1

# 配置 Channel
agent1.channels.ch1.type = memory
agent1.channels.ch1.capacity = 1000
agent1.channels.ch1.transactionCapacity = 100

</code></pre><ol start=2><li>启动 Agent agent1，命令如下。</li></ol><pre><code>flume-ng agent --conf /opt/flume/conf/ --conf-file /opt/flume/conf/avro.conf --name agent1 -Dflume.root.logger=INFO,console
</code></pre><ol start=3><li>用 XShell 重新打开一个新终端窗口，创建 avro-input1.txt 输入一些信息。</li></ol><pre><code>echo &quot;hello flume&quot; &gt; ~/avro-input1.txt
</code></pre><ol start=4><li>在新终端窗口使用 avro-client 向 agent1 监听的 avro 服务发送文件。</li></ol><pre><code>flume-ng avro-client -c /opt/flume/conf/ -H 0.0.0.0 -p 4141 -F ~/avro-input1.txt
</code></pre><ol start=5><li>在第一个终端窗口输出信息的最后一行可看到发送文件的内容。</li></ol><pre><code>2021-06-07 17:00:36,006 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 68 65 6C 6C 6F 20 66 6C 75 6D 65                hello flume }
</code></pre><h2 id=实验-103-flume-的配置与使用-syslogsource---memorychannel---hdfssink>实验 10.3 Flume 的配置与使用 Syslog(Source) -> Memory(Channel) -> HDFS(Sink)</h2><h3 id=实验名称flume-的配置与使用-2>【实验名称】Flume 的配置与使用 2</h3><h3 id=实验目的-2>【实验目的】</h3><p>1．理解 Flume 的基本原理，掌握各组件的作用及关系。
2．熟悉 Flume 的常用配置。</p><h3 id=实验原理-1>【实验原理】</h3><p>Syslog Source 读取 syslog 数据，产生 Event，syslog 支持 UDP 和 TCP 协议。通过 Memory Channel，把 Event 写入 HDFS。</p><h3 id=实验环境-2>【实验环境】</h3><ul><li>Windows 7 以上64位操作系统</li><li>JDK8</li><li>VMWare Workstation Pro</li><li>Hadoop 2.7.3</li><li>CentOS 7</li><li>Flume 1.8.0 （下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz）</li></ul><h3 id=实验资源-2>【实验资源】</h3><pre><code>下载链接：https://pan.baidu.com/s/1ghde86wcK6pwg1fdSSWg0w 
提取码：v3wv 
</code></pre><h3 id=实验步骤-2>【实验步骤】</h3><ol><li>创建 Agent 配置文件/opt/flume/conf/syslogtcp.conf，并按如下配置。注意替换为你的学号。</li></ol><pre><code>vim /opt/flume/conf/syslogtcp.conf
</code></pre><pre><code>agent2.sources = src2
agent2.sinks = sink2
agent2.channels = ch2
# 配置 Source
agent2.sources.src2.type = syslogtcp
agent2.sources.src2.port = 5140
agent2.sources.src2.host = localhost
agent2.sources.src2.channels = ch2
# 配置 Sink
agent2.sinks.sink2.type = hdfs
#注意这里的 HDFS 的IP地址替换为你的真实IP地址
agent2.sinks.sink2.hdfs.path = hdfs://nodea你的学号后4位:8020/user/hadoop/flume/syslogtcp输入你的学号后4位
agent2.sinks.sink2.hdfs.filePrefix = Syslog
agent2.sinks.sink2.hdfs.round = true
agent2.sinks.sink2.hdfs.roundValue = 10
agent2.sinks.sink2.hdfs.roundUnit = minute
agent2.sinks.sink2.channel = ch2

# 配置 Channel
agent2.channels.ch2.type = memory
agent2.channels.ch2.capacity = 1000
agent2.channels.ch2.transactionCapacity = 100
# 绑定 Source 和 Sink 到 Channel
</code></pre><ol start=2><li>启动 HDFS。</li></ol><pre><code>start-dfs.sh
</code></pre><ol start=3><li>启动 Agent agent2，命令如下。</li></ol><pre><code>flume-ng agent -c /opt/flume/conf/ -f /opt/flume/conf/syslogtcp.conf -n agent2 -Dflume.root.logger=INFO,console
</code></pre><ol start=4><li>XShell 启动一个新的终端窗口，输入以下命令，测试产生 syslog。请注意以下命令需要替换为你的个人学号。</li></ol><pre><code>echo &quot;hello 替换为你的学号&quot; | nc localhost 5140
</code></pre><blockquote><p>注1：关于nc（netcat）命令，大家可以搜索“Linux netcat 命令”或参考 <a href=http://www.linuxso.com/command/nc.html>http://www.linuxso.com/command/nc.html</a></p><p>注2：关于 | （管道）命令，大家可以搜索“Linux 管道 命令”或参考 <a href=https://blog.csdn.net/hellojoy/article/details/77337854>https://blog.csdn.net/hellojoy/article/details/77337854</a></p></blockquote><ol start=5><li>在新的终端窗口查看 HDFS 相应配置的路径上是否生成了 syslogtcp 文件，并查看文件内容，正确能看到“hello 你的学号”。（请注意以下命令需要替换为你的个人学号。）</li></ol><pre><code>hdfs dfs -ls /user/hadoop/flume/syslogtcp你的学号后4位
</code></pre><pre><code>hdfs dfs -cat /user/hadoop/flume/syslogtcp你的学号后4位/Syslog.xxxxxx
</code></pre><h2 id=实验-104-sqoop-的安装>实验 10.4 Sqoop 的安装</h2><h3 id=实验名称实验-104-sqoop-的安装>【实验名称】实验 10.4 Sqoop 的安装</h3><h3 id=实验目的-3>【实验目的】</h3><ul><li>熟悉 Sqoop 的安装</li></ul><h3 id=实验环境-3>【实验环境】</h3><ul><li>Windows 7 以上64位操作系统</li><li>JDK8</li><li>VMWare Workstation Pro</li><li>Hadoop 2.7.3</li><li>CentOS 7</li><li>Sqoop 1.4.7 （下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz）</li></ul><h3 id=实验资源-3>【实验资源】</h3><pre><code>下载链接：https://pan.baidu.com/s/1ghde86wcK6pwg1fdSSWg0w 
提取码：v3wv 
</code></pre><h3 id=实验步骤-3>【实验步骤】</h3><ol><li>在NodeA节点运行以下语句，注意提升为root权限执行。</li></ol><ul><li>创建 Sqoop 的安装目录。</li></ul><pre><code>su
mkdir /opt/sqoop
chown hadoop:wheel /opt/flume
</code></pre><ul><li>提升 root 用户权限执行以下语句，加入 ZooKeeper 环境变量。</li></ul><pre><code>su
echo &quot;export SQOOP_HOME=/opt/sqoop
export PATH=\$SQOOP_HOME/bin:\$PATH&quot; &gt;&gt;/etc/profile
</code></pre><ul><li>切换 hadoop 用户</li></ul><pre><code>su hadoop
</code></pre><ul><li>使环境变量生效。</li></ul><pre><code>source /etc/profile
</code></pre><ol start=2><li>使用 hadoop 登录NodeA节点。</li></ol><pre><code>su hadoop
</code></pre><ol start=3><li><p>上传 Sqoop 安装包<code>sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</code> 到 NodeA 节点 <code>/home/hadoop</code> 目录。</p></li><li><p>解压<code>sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</code>到<code>/home/hadoop</code> 目录。</p></li></ol><pre><code>tar -xvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
</code></pre><ol start=5><li>把解压以后的目录移到安装目录</li></ol><pre><code>sudo mv ~/sqoop-1.4.7.bin__hadoop-2.6.0/* /opt/sqoop
</code></pre><ol start=6><li>查看 Sqoop 版本，验证 Sqoop 是否正确安装</li></ol><pre><code>sqoop version
</code></pre><ol start=7><li><p>上传 MySQL 的驱动包 <code>mysql-connector-java-5.1.48.jar</code> 到<code>/home/hadoop</code> 目录。</p></li><li><p>拷贝驱动包到 Sqoop 的 lib 目录下。</p></li></ol><pre><code>mv ~/mysql-connector-java-5.1.48.jar $SQOOP_HOME/lib
</code></pre><ol start=9><li>拷贝 Hive 的依赖包到 Sqoop 的 lib 目录下。</li></ol><pre><code>cp $HIVE_HOME/lib/hive-common-2.3.8.jar $SQOOP_HOME/lib/ 
</code></pre><h2 id=实验-105-使用-sqoop-进行数据转换>实验 10.5 使用 Sqoop 进行数据转换</h2><h3 id=实验名称105-使用-sqoop-进行数据转换>【实验名称】10.5 使用 Sqoop 进行数据转换</h3><h3 id=实验目的-4>【实验目的】</h3><ul><li>掌握使用 Sqoop 在 MariaDB、Hive、HDFS、HBase 之间进行数据转换</li></ul><h3 id=实验环境-4>【实验环境】</h3><ul><li>Windows 7 以上64位操作系统</li><li>JDK8</li><li>VMWare Workstation Pro</li><li>Hadoop 2.7.3</li><li>CentOS 7</li><li>Sqoop 1.4.7 （下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz）</li></ul><h3 id=实验资源-4>【实验资源】</h3><pre><code>下载链接：https://pan.baidu.com/s/1ghde86wcK6pwg1fdSSWg0w 
提取码：v3wv 
</code></pre><h3 id=实验步骤-4>【实验步骤】</h3><ol><li>使用 hadoop 登录NodeA节点。</li></ol><pre><code>su hadoop
</code></pre><ol start=2><li><p>上传员工表SQL脚本文件 <code>EMP.sql</code> 到 <code>/home/hadoop</code></p></li><li><p>使用 root 用户登录 MariaDB。</p></li></ol><pre><code>mysql -u root -p
</code></pre><ol start=4><li>在 MariaDB 中创建 sqoop 库和 sqoop 用户。</li></ol><pre><code>create database sqoopdb;
use sqoopdb；
create user 'sqoop'@'localhost' identified by 'sqoop123';
create user 'sqoop'@'%' identified by 'sqoop123';
grant all on sqoopdb.* to 'sqoop'@'localhost';
grant all on sqoopdb.* to 'sqoop'@'%';
</code></pre><ol start=5><li>执行 <code>EMP.sql</code> 文件 SQL 语句。</li></ol><pre><code>use sqoopdb；
source /home/hadoop/EMP.sql
</code></pre><ol start=6><li>在 MariaDB 查询员工表内容。</li></ol><pre><code>select * from EMP;
</code></pre><ol start=7><li>退出 MariaDB 的命令终端</li></ol><pre><code>exit
</code></pre><ol start=8><li>使用 Sqoop 对 MariaDB 的数据库进行查询。是否存在库 <code>sqoopdb</code></li></ol><pre><code>sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root -password 123456
</code></pre><ol start=9><li>使用 Sqoop 对 MariaDB 的数据库 sqoopdb 进行查询。是否存在表<code>EMP</code>。</li></ol><pre><code>sqoop list-tables --connect jdbc:mysql://localhost:3306/sqoopdb --username sqoop -password sqoop123
</code></pre><h3 id=实验步骤--导入数据从-mariadb-到-hdfs>【实验步骤- 导入数据从 MariaDB 到 HDFS】</h3><ol start=10><li>启动 Hadoop。</li></ol><pre><code>start-hdp.sh
</code></pre><ol start=11><li>把 MariaDB 中的 <code>EMP</code>表导出到 HDFS 的 <code>/sqoop</code>目录。其中参数 <code>-m 1</code> 表示使用1个 Mapper。注意替换为你的学号后4位。</li></ol><pre><code>hdfs dfs -rm -r /sqoop 
sqoop import --connect jdbc:mysql://nodea你的学号后4位:3306/sqoopdb --username sqoop -password sqoop123 --table EMP -target-dir /sqoop -m 1
</code></pre><ol start=12><li>输出导入的结果进行查看</li></ol><pre><code>hdfs dfs -cat /sqoop/part-m-00000
</code></pre><h3 id=实验步骤--导入数据从-mariadb-到-hive>【实验步骤- 导入数据从 MariaDB 到 Hive】</h3><ol start=13><li>把 MariaDB 的<code>EMP</code>表导入到 Hive。注意替换为你的学号。</li></ol><pre><code>sqoop import --connect jdbc:mysql://nodea你的学号后4位:3306/sqoopdb --username sqoop -password sqoop123 --table EMP --fields-terminated-by '\t' --target-dir /user/hadoop/db2hive --num-mappers 1 --hive-database default --hive-import --hive-table emp
</code></pre><ol start=14><li>进入 Hive，并查询 emp 表的内容。</li></ol><pre><code>hive
</code></pre><pre><code>hive (default)&gt; use default;
hive (default)&gt; select * from emp;
</code></pre><h3 id=实验步骤--导入数据从-mariadb-到-hbase>【实验步骤- 导入数据从 MariaDB 到 HBase】</h3><ol start=15><li>启动 HBase，并登录 HBase。</li></ol><pre><code>start-dfs.sh
start-hbase.sh
hbase shell
</code></pre><ol start=16><li>创建一个 HBase 表 EMP。</li></ol><pre><code>create 'EMP', { NAME =&gt; 'EMPINFO', VERSIONS =&gt; 5}
</code></pre><blockquote><p>注：上面命令在 HBase 中创建了一个 EMP 表，这个表中有一个列族 EMPINFO，历史版本保留数量为 5。</p></blockquote><ol start=17><li>创建完成，通过命令 list 可看到 HBase 中有表 EMP。</li></ol><pre><code>list
</code></pre><ol start=18><li>退出HBase Shell。</li></ol><pre><code>quit
</code></pre><ol start=19><li>执行以下命令把 MariaDB 中的 EMP 表导入到 HBase 中的 EMP 表，并使用 <code>EMPNO</code> 作为 <code>Row key</code>。注意替换为你的学号。</li></ol><pre><code>sqoop import --connect jdbc:mysql://nodea你的学号后4位:3306/sqoopdb --username sqoop --password sqoop123 --table EMP --hbase-table EMP --column-family EMPINFO --hbase-row-key EMPNO
</code></pre><ol start=20><li>再次进入 Hbase Shell</li></ol><pre><code>hbase shell
</code></pre><ol start=21><li>查看 EMP 表的数据</li></ol><pre><code>scan 'EMP'
</code></pre><h3 id=实验步骤--导出数据从-hdfs-到-mariadb>【实验步骤- 导出数据从 HDFS 到 MariaDB】</h3><ol start=22><li>在导出前需要在 MariaDB 中创建接收 HDFS 数据的空表<code>EMP2</code>。登录 sqp。</li></ol><pre><code>mysql sqoopdb -u sqoop -psqoop123
</code></pre><ol start=23><li>创建接收数据的空表 <code>EMP2</code></li></ol><pre><code>create table `EMP2` like `EMP`;
</code></pre><ol start=24><li>对比<code>EMP</code>和<code>EMP2</code>的表结构。</li></ol><pre><code>describe `EMP2`;
describe `EMP`;
</code></pre><ol start=25><li>通过以下命令把之前导入到 HDFS 的数据，再次导出到 MariaDB 的 EMP2 表。注意替换为你的学号。</li></ol><pre><code>sqoop export --connect jdbc:mysql://nodea你的学号后4位:3306/sqoopdb --table EMP2 --export-dir /sqoop/part-m-00000 --username sqoop --password sqoop123 -m 1
</code></pre><ol start=26><li>导出成功后可进入 MariaDB 查看导出的内容。</li></ol><pre><code>select * from  `EMP2`;
</code></pre></div><button id=scroll-top-btn>↑顶部</button><div id=outerdiv style=position:fixed;top:0;left:0;background:rgba(0,0,0,.7);z-index:2;width:100%;height:100%;display:none><div id=innerdiv style=position:absolute><img id=bigimg style="border:5px solid #fff" src></div><script src=https://huangzhiyi.github.io/js/highlight.pack.js></script><script src=https://huangzhiyi.github.io/js/highlightjs-line-numbers.min.js></script><script>hljs.initHighlightingOnLoad();hljs.initLineNumbersOnLoad();$(function(){$('div.tbl-start').nextUntil('div.tbl-end','table').addClass('tbl');$("img").click(function(){var _this=$(this);imgShow("#outerdiv","#innerdiv","#bigimg",_this);});$("#scroll-top-btn").click(function(){document.body.scrollTop=document.documentElement.scrollTop=0;});});</script><script src=https://huangzhiyi.github.io/js/clipboard.min.js></script><script src=https://huangzhiyi.github.io/js/codecopy.bundle.js?20210612></script></main><script src=https://huangzhiyi.github.io/js/jquery-min.js></script><script src=https://huangzhiyi.github.io/js/heis-custom-min.js?210610></script></body></html>
<!doctype html><html lang=zh><meta charset=utf-8><meta name=viewport content="width=device-width"><title>P2 - HDFS 实战 | Hadoop集群部署与开发 V5 | Heis</title><meta name=generator content="Hugo Eureka 0.8.4"><link rel=stylesheet href=/css/eureka.min.css><script defer src=/js/eureka.min.js></script><script src=/static/js/jquery-3.6.0.min.js></script><script src=/static/js/heis.js?230321></script><link rel=stylesheet href=/static/css/heis.css?230321 media=all onload="this.media='all';this.onload=null" crossorigin><link rel=stylesheet href=/static/css/print.css?230413 media=print crossorigin><link rel=stylesheet href=/static/css/highlight-css/solarized-light.min.css media=print onload="this.media='all';this.onload=null" crossorigin><script defer src=/static/js/highlight.min.js crossorigin></script><script defer src=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js integrity="sha256-uNYoXefWRqv+PsIF/OflNmwtKM4lStn9yrz2gVl6ymo=" crossorigin></script><script defer src=/static/js/highlightjs-line-numbers.min.js></script><link rel=icon type=image/png sizes=32x32 href=/images/icon_huddfe7d1590fd89155f46b33625956ea5_4357_32x32_fill_box_center_2.png><link rel=apple-touch-icon sizes=180x180 href=/images/icon_huddfe7d1590fd89155f46b33625956ea5_4357_180x180_fill_box_center_2.png><meta name=description content="掌握通过 Shell 命令访问和管理 HDFS"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"系列文章","item":"/docs/"},{"@type":"ListItem","position":2,"name":"Hadoop集群部署与开发 V5","item":"/docs/hadoop-e/"},{"@type":"ListItem","position":3,"name":"P2 - HDFS 实战","item":"/docs/hadoop-e/hadoop-e02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"/docs/hadoop-e/hadoop-e02/"},"headline":"P2 - HDFS 实战 | Hadoop集群部署与开发 V5 | Heis","datePublished":"2025-02-13T00:44:00+08:00","dateModified":"2025-02-13T00:44:00+08:00","wordCount":3524,"author":{"@type":"Person","name":"heis"},"publisher":{"@type":"Person","name":"C. Wang","logo":{"@type":"ImageObject","url":"/images/icon.png"}},"description":"掌握通过 Shell 命令访问和管理 HDFS"}</script><meta property="og:title" content="P2 - HDFS 实战 | Hadoop集群部署与开发 V5 | Heis"><meta property="og:type" content="article"><meta property="og:image" content="/images/icon.png"><meta property="og:url" content="/docs/hadoop-e/hadoop-e02/"><meta property="og:description" content="掌握通过 Shell 命令访问和管理 HDFS"><meta property="og:locale" content="zh"><meta property="og:site_name" content="Heis"><meta property="article:published_time" content="2025-02-13T00:44:00+08:00"><meta property="article:modified_time" content="2025-02-13T00:44:00+08:00"><meta property="article:section" content="docs"><meta property="article:tag" content="hadoop"><meta property="article:tag" content="大数据"><meta property="article:tag" content="课程"><meta property="og:see_also" content="/docs/hadoop-e/hadoop-e90/"><meta property="og:see_also" content="/docs/hadoop-e/hadoop-e05/"><meta property="og:see_also" content="/docs/hadoop-e/hadoop-e04/"><meta property="og:see_also" content="/docs/hadoop-e/hadoop-e03/"><meta property="og:see_also" content="/docs/hadoop-e/hadoop-e01/"><meta property="og:see_also" content="/docs/hadoop-e/hadoop-cmd/"><script>var _hmt=_hmt||[];(function(){var hm=document.createElement("script");hm.src="https://hm.baidu.com/hm.js?f58b6a2675cf52000232edb4d109eccc";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script><body class="flex flex-col min-h-screen"><header id=headerctn class="fixed flex items-center w-full pl-scrollbar z-50 bg-secondary-bg shadow-sm"><div class="w-full max-w-screen-xl mx-auto"><script>let storageColorScheme=localStorage.getItem("lightDarkMode")
if(((storageColorScheme=='Auto'||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches)||storageColorScheme=="Dark"){document.getElementsByTagName('html')[0].classList.add('dark')}</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="mr-6 text-primary-text text-xl font-bold">Heis</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent mr-4">文章</a>
<a href=/docs/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item mr-4">系列</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>浅色</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>深色</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>自动</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById('lightDarkMode')
if(storageColorScheme==null||storageColorScheme=='Auto'){document.addEventListener('DOMContentLoaded',()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change',switchDarkMode)})}else if(storageColorScheme=="Light"){element.firstElementChild.classList.remove('fa-adjust')
element.firstElementChild.setAttribute("data-icon",'sun')
element.firstElementChild.classList.add('fa-sun')}else if(storageColorScheme=="Dark"){element.firstElementChild.classList.remove('fa-adjust')
element.firstElementChild.setAttribute("data-icon",'moon')
element.firstElementChild.classList.add('fa-moon')}
document.addEventListener('DOMContentLoaded',()=>{getcolorscheme();switchBurger();});</script></div></header><main class="flex-grow pt-16"><div class=pl-scrollbar><div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto"><div id=doc-container class=lg:pt-12><div class="flex flex-col md:flex-row bg-secondary-bg rounded"><div class="md:w-1/4 lg:w-1/5 border-r"><div class="sticky top-16 pt-6"><div id=sidebar-title class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text"><span class=font-semibold>目录</span>
<i class="fas fa-caret-right ml-1"></i></div><div id=sidebar-toc class="hidden md:block overflow-y-auto mx-6 md:mx-0 pr-6 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent"><div class="flex flex-wrap ml-4 -mr-2 p-2 bg-secondary-bg md:bg-primary-bg rounded"><a class=hover:text-eureka href=/docs/hadoop-e/>Hadoop集群部署与开发 V5</a></div><ul class=pl-6><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-e01/>P1 - 部署 Hadoop 完全分布式与搭建开发环境</a></div></li><li class=py-2><div><a class="text-eureka hover:text-eureka" href=/docs/hadoop-e/hadoop-e02/>P2 - HDFS 实战</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-e03/>P3 - MapReduce实战</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-e04/>P4 - Hive 部署与实践</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-e05/>P5 - HBase 部署与实践</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-e06/>P6 - Zookeeper 部署与实践</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-e07/>P7 - Flume 和 Sqoop 部署与实践</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-e90/>Hadoop 模板机制作</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-faq/>常见问题</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-e/hadoop-cmd/>常用命令</a></div></li></ul></div></div></div><div class="w-full md:w-3/4 lg:w-4/5 pb-8 pt-2 md:pt-8"><div id=doc-tit class="w-full lg:w-3/4 pl-6 ml-0 mr-auto"><h1 class="font-bold text-3xl text-primary-text">P2 - HDFS 实战</h1><div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text"><div class="mr-6 my-2"><i class="fas fa-calendar mr-1"></i><span>2025-02-13</span></div><div class="mr-6 my-2"><i class="fas fa-clock mr-1"></i><span>8分钟阅读时长</span></div><div class="mr-6 my-2"><i class="fas fa-folder mr-1"></i><a href=/categories/hadoopd/ class=hover:text-eureka>hadoopd</a></div></div></div><div class=flex><div class="w-full lg:w-3/4 px-6"><div class=content><h2 id=版本>【版本】</h2><p>当前版本号<code>v20250213</code></p><div class=tbl-start></div><table><thead><tr><th>版本</th><th>修改说明</th></tr></thead><tbody><tr><td>v20250213</td><td>初始化版本</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h2 id=任务21-通过-shell-命令访问和管理-hdfs>任务2.1 通过 Shell 命令访问和管理 HDFS</h2><h2 id=任务目的>【任务目的】</h2><ul><li>掌握 Web Console 访问 HDFS。</li><li>掌握常用的 Shell 命令访问 HDFS。</li></ul><h2 id=任务环境>【任务环境】</h2><ul><li>内存：至少4G</li><li>硬盘：至少空余40G</li><li>操作系统: CentOS 7.9</li></ul><h2 id=任务资源>【任务资源】</h2><ul><li>FinalShell</li><li>VirtualBox</li><li>Hadoop 完全分布式环境</li></ul><h2 id=任务内容>【任务内容】</h2><ul><li>使用 HDFS 的相关命令完成相关要求</li></ul><h2 id=任务参考>【任务参考】</h2><ul><li>hdfs dfs 操作命令</li></ul><pre><code>hadoop fs [generic options]
	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]
	[-cat [-ignoreCrc] &lt;src&gt; ...]
	[-checksum &lt;src&gt; ...]
	[-chgrp [-R] GROUP PATH...]
	[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]
	[-chown [-R] [OWNER][:[GROUP]] PATH...]
	[-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]
	[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]
	[-count [-q] [-h] &lt;path&gt; ...]
	[-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]
	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]
	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]
	[-df [-h] [&lt;path&gt; ...]]
	[-du [-s] [-h] &lt;path&gt; ...]
	[-expunge]
	[-find &lt;path&gt; ... &lt;expression&gt; ...]
	[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]
	[-getfacl [-R] &lt;path&gt;]
	[-getfattr [-R] {-n name | -d} [-e en] &lt;path&gt;]
	[-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]
	[-help [cmd ...]]
	[-ls [-d] [-h] [-R] [&lt;path&gt; ...]]
	[-mkdir [-p] &lt;path&gt; ...]
	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]
	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]
	[-mv &lt;src&gt; ... &lt;dst&gt;]
	[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]
	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]
	[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]
	[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]
	[-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]
	[-setfattr {-n name [-v value] | -x name} &lt;path&gt;]
	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]
	[-stat [format] &lt;path&gt; ...]
	[-tail [-f] &lt;file&gt;]
	[-test -[defsz] &lt;path&gt;]
	[-text [-ignoreCrc] &lt;src&gt; ...]
	[-touchz &lt;path&gt; ...]
	[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]
	[-usage [cmd ...]]
</code></pre><ul><li>HDFS 安全模式操作</li></ul><pre><code>enter - 进入安全模式
leave - 强制NameNode离开安全模式
get   - 返回安全模式是否开启的信息
wait  - 等待，一直到安全模式结束。
</code></pre><ul><li>HDFS 开启关闭快照功能</li></ul><pre><code>#在某个目录开启快照功能
hdfs dfsadmin -allowSnapshot &lt;snapshotDir&gt;
#在某个目录关闭快照功能
hdfs dfsadmin -disallowSnapshot &lt;snapshotDir&gt;
</code></pre><ul><li>HDFS 操作快照功能</li></ul><pre><code>#创建快照
hdfs dfs -createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]
#删除快照
hdfs dfs -deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;
#重命名快照
hdfs dfs -renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;
</code></pre><ul><li>文件目录数量配额（Quota）
setQuota指的是对HDFS中某个目录设置文件和目录数量之和的最大值。</li></ul><pre><code>hdfs dfsadmin -setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;
</code></pre><ul><li>空间配额（SpaceQuota）
setSpaceQuota针对的是设置HDFS中某个目录可用存储空间大小，单位是byte。在使用该命令的时候最好设置空间大小为数据块（默认128MB）的整数倍。</li></ul><pre><code>hdfs dfsadmin -setSpaceQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;
</code></pre><ul><li>报告文件系统的基本信息和统计信息</li></ul><pre><code>hdfs dfsadmin -report
</code></pre><ul><li>查看拓扑</li></ul><pre><code>hdfs dfsadmin -printTopology
</code></pre><h2 id=任务步骤>【任务步骤】</h2><ol><li>启动NodeA、NodeB、NodeC3个节点的虚拟机，使用<code>hadoop</code>用户登录 <code>NodeA</code>，启动 Hadoop。</li></ol><pre><code>start-hdp.sh
</code></pre><h3 id=hdfs-机架拓扑>HDFS 机架拓扑</h3><ol start=2><li>查看 HDFS 的机架拓扑。</li></ol><pre><code>hdfs dfsadmin -printTopology
</code></pre><ol start=3><li>查看 put 命令的使用方法和参数</li></ol><pre><code>hdfs dfs -help put
</code></pre><h3 id=hdfs-文件操作>HDFS 文件操作</h3><ol start=4><li>在用户目录下创建一个文本文件。注意替换为你的学号。</li></ol><pre><code>cd ~
echo &quot;uploaded by 你的学号后4位&quot; &gt;uploader.txt
ls
</code></pre><ol start=5><li>使用<code>hdfs dfs -put</code>命令上传本地<code>~/uploader.txt</code>文件到 HDFS 的根目录<code>/</code>下。</li></ol><h3 id=hdfs-文件目录操作>HDFS 文件目录操作</h3><ol start=6><li><p>查看<code>hdfs dfs -mkdir</code>的用法，并在 HDFS 上创建<code>/poetry</code>文件夹。</p></li><li><p>从实验资源下载<code>haizi1.txt</code>到本地~目录。</p></li><li><p>使用<code>hdfs dfs -moveFromLocal</code>或者<code>hdfs dfs -put</code>命令，上传<code>haizi1.txt</code>到 HDFS 的<code>/poetry</code>目录下。</p></li><li><p>查看 HDFS 的<code>/poetry</code>目录，确认是否上传成功。</p></li></ol><pre><code>hdfs dfs -ls -h /poetry
</code></pre><h3 id=hdfs-文件内容操作>HDFS 文件内容操作</h3><ol start=10><li><p>使用<code>hdfs dfs -cat</code>命令输出HDFS 的<code>/poetry/haizi1.txt</code>文件的内容。</p></li><li><p>使用<code>hdfs dfs -cp</code>命令把 HDFS 的<code>/poetry/haizi1.txt</code>复制为<code>/poetry/FacingTheSea.txt</code>。</p></li><li><p>使用<code>hdfs dfs -checksum</code>命令分别输出<code>/poetry/haizi1.txt</code>和<code>/poetry/FacingTheSea.txt</code>。比较两者的校验码是否一致。</p></li><li><p>使用<code>hdfs dfs -appendToFile</code>命令把本地的<code>~/uploader.txt</code>的内容追加到 HDFS 的<code>/poetry/FacingTheSea.txt</code></p></li><li><p>使用<code>hdfs dfs -tail</code>命令输出 HDFS 的<code>/poetry/FacingTheSea.txt</code>的内容。</p></li><li><p>使用<code>hdfs dfs -rm</code>命令移除 HDFS 的<code>/poetry/haizi1.txt</code>，并查看<code>/poetry</code>目录下的文件，确认是否删除成功。</p></li><li><p>使用<code>hdfs dfs -mv</code>命令移动 HDFS 的<code>/poetry/FacingTheSea.txt</code>到根目录下。</p></li><li><p>使用<code>hdfs dfs -rmdir</code>命令删除 HDFS 的<code>/poetry</code>目录。</p></li><li><p>使用<code>hdfs dfs -find</code>找出 HDFS 根目录下的所有<code>.txt</code>结尾的文件。</p></li><li><p>使用<code>hdfs dfs -ls -R /</code>列出 HDFS 根目录下的所有文件。</p></li></ol><h3 id=hdfs-安全模式操作>HDFS 安全模式操作</h3><ol start=20><li>使用命令让 HDFS 进入安全模式</li></ol><pre><code>hdfs dfsadmin -safemode enter
</code></pre><ol start=21><li>尝试创建一个文件，观察 HDFS 提示信息。</li></ol><pre><code>hdfs dfs -touchz /test.txt
</code></pre><ol start=22><li>获取安全模式的状态。</li></ol><pre><code>hdfs dfsadmin -safemode get
</code></pre><ol start=23><li>使用相应命令退出安全模式。</li></ol><h3 id=hdfs-快照操作>HDFS 快照操作</h3><ol start=24><li><p>在 HDFS 创建目录<code>/snapshot+你学号后3位</code>，注意替换为你的学号。</p></li><li><p>在该目录上开启快照（snapshot）功能，注意替换为你的学号。</p></li></ol><pre><code>hdfs dfsadmin -allowSnapshot /snapshot+你学号后3位
</code></pre><ol start=26><li><p>上传本地的<code>haizi1.txt</code>文件到<code>/snapshot+你学号后3位</code>目录</p></li><li><p>创建名为<code>v1</code>的快照，注意替换为你的学号。</p></li></ol><pre><code>hdfs dfs -createSnapshot /snapshot+你学号后3位 v1
</code></pre><ol start=28><li><p>使用<code>hdfs dfs -appendToFile</code>把本地的<code>haizi2.txt</code>追加到<code>/snapshot+你学号后3位/haizi1.txt</code>文件。</p></li><li><p>使用<code>diff</code>命令对比快照修改前后的内容。</p></li></ol><pre><code>diff &lt;(hdfs dfs -cat /snapshot+你学号后3位/.snapshot/v1/haizi1.txt) &lt;(hdfs dfs -cat /snapshot+你学号后3位/haizi1.txt)
</code></pre><h3 id=hdfs-容量操作>HDFS 容量操作</h3><ol start=30><li><p>在 HDFS 上创建目录<code>/quota+你学号后3位</code>，注意替换为你的学号。</p></li><li><p>设置文件/目录数量配额为2，注意替换为你的学号。</p></li></ol><pre><code>hdfs dfsadmin -setQuota 2 /quota+你学号后3位
</code></pre><ol start=32><li>尝试在<code>/quota+你学号后3位</code>目录下创建2个文件，观察会有什么反馈消息。</li></ol><h2 id=任务22-通过编程接口访问和管理-hdfs>任务2.2 通过编程接口访问和管理 HDFS</h2><h2 id=任务目的-1>【任务目的】</h2><ul><li>掌握如何使用 IDEA 创建 Maven 工程、运行 Maven 工程。</li><li>掌握 HDFS 文件系统的 Java 编程接口的调用和编程</li></ul><h2 id=任务环境-1>【任务环境】</h2><ul><li>Windows 7 以上64位操作系统</li><li>JDK 8</li><li>Maven 3</li><li>IDEA 社区版</li></ul><h2 id=任务资源-1>【任务资源】</h2><ul><li>hadoop.dll</li></ul><h2 id=任务内容-1>【任务内容】</h2><ul><li>按要求使用 Java 编程实现访问 HDFS</li></ul><h2 id=任务参考-1>【任务参考】</h2><p>类 org.apache.hadoop.fs.FileSystem 的 <a href=https://hadoop.apache.org/docs/r3.3.1/api/org/apache/hadoop/fs/FileSystem.html>参考文档</a></p><ul><li>FileSystem.copyToLocalFile 方法用于从 HDFS 复制文件到本地文件系统。<a href=https://hadoop.apache.org/docs/r3.3.1/api/org/apache/hadoop/fs/FileSystem.html#copyToLocalFile-org.apache.hadoop.fs.Path-org.apache.hadoop.fs.Path->详细参考文档</a></li></ul><pre><code>/**
 * @Param src HDFS 文件路径
 * @Param dst 本地文件路径
 */
public void copyToLocalFile(Path src, Path dst) throws IOException
</code></pre><ul><li>FileSystem.copyFromLocalFile 方法用于从本地文件系统复制文件到 HDFS。<a href=https://hadoop.apache.org/docs/r3.3.1/api/org/apache/hadoop/fs/FileSystem.html#copyFromLocalFile-org.apache.hadoop.fs.Path-org.apache.hadoop.fs.Path->详细参考文档</a></li></ul><pre><code>/**
 * @Param src 本地文件路径
 * @Param dst HDFS 文件路径
 */
public void copyFromLocalFile(Path src, Path dst) throws IOException
</code></pre><h2 id=任务要求>【任务要求】</h2><ol><li><p>打开 Part 1 的 <code>hadoop-exp</code> 项目。</p></li><li><p>在项目<code>hadoop-exp\src\main\java</code>下创建一个名为<code>hadoop+你学号后3位.hdfs</code>的包。注意替换为你的学号后3位。</p></li><li><p>在包下面新建一个 HdfsUtils 的类。该类包含了一个在HDFS上创建文件并写入内容的方法<code>createFile</code>。代码如下，注意替换为你的学号后3位。</p></li></ol><pre><code>package hadoop你学号**后3位**.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;

public class HdfsUtils {

    /**
     * 在HDFS上创建文件并写入内容
     * @param conf HDFS的配置
     * @param hdfsFilePath HDFS创建的文件的路径
     * @param content 写入文件的内容
     * @param overwrite true表示覆盖原文件，false表示不覆盖
     */
    public static boolean createFile(Configuration conf,URI uri,String hdfsFilePath,String content,boolean overwrite) {
        FileSystem fs=null;
        FSDataOutputStream os=null;
        boolean rs=false;
        try {
            // 指定用户名 , 获取 FileSystem 对象
            fs = FileSystem.get(uri, conf, &quot;hadoop&quot;);
            //定义文件路径
            Path dfs = new Path(hdfsFilePath);
            os = fs.create(dfs, overwrite);
            //往文件写入信息
            os.writeBytes(content);
            //成功创建文件，设置为true
            rs=true;
        }catch(Exception e){
            e.printStackTrace();
        }finally {
            try {
                if(os!=null) {
                    // 关闭流
                    os.close();
                }
                if(fs!=null) {
                    // 不需要再操作 FileSystem 了，关闭
                    fs.close();
                }
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        return rs;
    }
}
</code></pre><ol start=4><li><p>在项目<code>hadoop-exp\src\test\java</code>下创建一个名为<code>hadoop+你学号后3位.hdfs</code>的包。注意替换为你的学号后3位。</p></li><li><p>在包下面新建一个 HdfsUtilsTest 的类，用于单元测试。代码如下，注意替换为你的学号后3位。此测试用例主要目的是在 HDFS 上创建一个文件，并把 “hello” 写入文件。</p></li></ol><pre><code>package hadoop替换为你学号后3位.hdfs;

import hadoop替换为你学号后3位.hdfs.HdfsUtils;
import org.apache.hadoop.conf.Configuration;
import org.junit.Test;

import java.net.URI;

public class HdfsUtilsTest {
    @Test
    public void testCreate() {
        Configuration conf=new Configuration();
        boolean rs=false;
        //创建的文件路径
        String filePath=&quot;/替换为你学号后3位/test.txt&quot;;
        String content=&quot;hello&quot;;
        try {
            URI uri = new URI(&quot;hdfs://10.0.0.71:8020&quot;);
            rs= HdfsUtils.createFile(conf, uri, filePath, content, true);
        }catch(Exception e){
            e.printStackTrace();
        }
        if(rs){
            System.out.println(&quot;Create successfully!&quot;);
        }else{
            System.out.println(&quot;Create fail!&quot;);
        }
    }
}
</code></pre><ol start=6><li>复制文件 <code>hadoop.dll</code> 到 <code>C:\Windows\System32</code> 下。</li></ol><div class=warning>重要步骤，请勿忽略！</div><ol start=7><li>启动虚拟机的 Hadoop。</li></ol><div class=warning>注意一定要启动 Hadoop，否则实验无法成功！</div><ol start=8><li><p>运行<code>HdfsUtilsTest</code>。查看 IDEA 的控制台输出结果是否创建成功。</p></li><li><p>SSH 登录 NodeA，运行以下命令查看创建文件的内容输出。是否和创建的文件内容一致。注意替换为你的学号后3位。</p></li></ol><pre><code>hdfs dfs -cat /替换为你学号后3位/test.txt
</code></pre><ol start=10><li>参考以上代码和【任务参考】给出的 API 文档，完成以下编程要求</li></ol><ul><li>（1）在<code>HdfsUtils</code>类新增一个<code>uploadFile</code>方法，可以上传本地文件到 HDFS。</li><li>（2）在<code>HdfsUtilsTest</code>类新增一个<code>testUpload</code>方法，测试上传文件到 HDFS。</li><li>（3）在<code>HdfsUtils</code>类新增一个<code>dowloadFile</code>方法，可以下载 HDFS 文件到本地。</li><li>（4）在<code>HdfsUtilsTest</code>类新增一个<code>testDownload</code>方法，测试下载 HDFS 文件到本地。</li></ul><h2 id=常见问题>【常见问题】</h2><h3 id=1-报错null-entry-in-command-string>1. 报错：(null) entry in command string</h3><p>答：这是缺少文件hadoop.dll文件</p><ul><li>1、下载<code>hadoop.dll</code>文件</li><li>2、把文件放在<code>c:\windows\system32</code>目录下</li></ul><h3 id=2-idea-报错javanetconnectionexceptioncall-from-xxxx10002-to-1000718020-failed-on-connection-exception>2. IDEA 报错：java.net.ConnectionException:Call From XXXX/10.0.0.2 to 10.0.0.71:8020 failed on connection exception.</h3><p>答：这是连接 NameNode 进程出错。</p><ul><li><ol><li>确保所有虚拟机节点都要启动。</li></ol></li><li><ol start=2><li>确保能够ping 到10.0.0.71</li></ol></li><li><ol start=3><li>确保 Hadoop 启动成功，使用jps命令检查进程是否缺少，特别是 namenode 进程。</li></ol></li></ul></div><div class=my-4><a href=/tags/hadoop/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#hadoop</a>
<a href=/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#大数据</a>
<a href=/tags/%E8%AF%BE%E7%A8%8B/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#课程</a></div><div class=py-2><div class="flex flex-col md:flex-row items-center my-8"><a href=/authors/heis/ class="w-24 h-24 md:mr-4"><img src=/static/img/authors/heis.png class="w-full bg-primary-bg rounded-full" alt=Avatar></a><div class="w-full md:w-auto mt-4 md:mt-0"><a href=/authors/heis/ class="block font-bold text-lg pb-1 mb-2 border-b">黄老师</a>
<span class="block pb-2"></span><a href=mailto:heishuangzy@qq.com class=mr-1><i class="fas fa-envelope"></i></a><a href=https://gitee.com/heis/ class=mr-1><i class="fab fa-git"></i></a><a href=http://heis.gitee.io/ class=mr-1><i class="fas fa-blog"></i></a></div></div></div><div id=btmQrcode></div><p style=text-align:center>扫码或长按识别访问</p><script src=/static/js/qrcode.min.js></script><script type=text/javascript>function getURL(){var url=window.location.href;if(window.location.hash!=""){url=url.substring(0,url.indexOf("#"));}
return url;}
new QRCode(document.getElementById("btmQrcode"),{text:getURL(),width:192,height:192,colorDark:"#000000",colorLight:"#ffffff",correctLevel:QRCode.CorrectLevel.H});</script><div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t"><div id=presec><span class="block font-bold">上一页</span>
<a href=/docs/hadoop-e/hadoop-e01/ class=block>P1 - 部署 Hadoop 完全分布式与搭建开发环境</a></div><div id=nextsec class="md:text-right mt-4 md:mt-0"><span class="block font-bold">下一页</span>
<a href=/docs/hadoop-e/hadoop-e03/ class=block>P3 - MapReduce实战</a></div></div></div><div class="hidden lg:block lg:w-1/4"><div class="sticky top-16 z-10 hidden lg:block px-6 py-4 bg-secondary-bg pt-16 -mt-16"><span class="text-lg font-semibold">本页内容</span></div><div class="sticky-toc hidden lg:block px-6 pb-6 pt-10 -mt-10 border-l"><nav id=TableOfContents><ul><li><a href=#版本>【版本】</a></li><li><a href=#任务21-通过-shell-命令访问和管理-hdfs>任务2.1 通过 Shell 命令访问和管理 HDFS</a></li><li><a href=#任务目的>【任务目的】</a></li><li><a href=#任务环境>【任务环境】</a></li><li><a href=#任务资源>【任务资源】</a></li><li><a href=#任务内容>【任务内容】</a></li><li><a href=#任务参考>【任务参考】</a></li><li><a href=#任务步骤>【任务步骤】</a><ul><li><a href=#hdfs-机架拓扑>HDFS 机架拓扑</a></li><li><a href=#hdfs-文件操作>HDFS 文件操作</a></li><li><a href=#hdfs-文件目录操作>HDFS 文件目录操作</a></li><li><a href=#hdfs-文件内容操作>HDFS 文件内容操作</a></li><li><a href=#hdfs-安全模式操作>HDFS 安全模式操作</a></li><li><a href=#hdfs-快照操作>HDFS 快照操作</a></li><li><a href=#hdfs-容量操作>HDFS 容量操作</a></li></ul></li><li><a href=#任务22-通过编程接口访问和管理-hdfs>任务2.2 通过编程接口访问和管理 HDFS</a></li><li><a href=#任务目的-1>【任务目的】</a></li><li><a href=#任务环境-1>【任务环境】</a></li><li><a href=#任务资源-1>【任务资源】</a></li><li><a href=#任务内容-1>【任务内容】</a></li><li><a href=#任务参考-1>【任务参考】</a></li><li><a href=#任务要求>【任务要求】</a></li><li><a href=#常见问题>【常见问题】</a><ul><li><a href=#1-报错null-entry-in-command-string>1. 报错：(null) entry in command string</a></li><li><a href=#2-idea-报错javanetconnectionexceptioncall-from-xxxx10002-to-1000718020-failed-on-connection-exception>2. IDEA 报错：java.net.ConnectionException:Call From XXXX/10.0.0.2 to 10.0.0.71:8020 failed on connection exception.</a></li></ul></li></ul></nav></div><script>window.addEventListener('DOMContentLoaded',()=>{enableStickyToc();});</script></div></div></div></div></div><div id=outerdiv><div id=innerdiv><img id=bigimg src></div></div><div id=popOuterDiv><div id=popDiv style=display:none><div class=close><span onclick="$('#popDiv').hide();"></span></div><p id=popCtn></p></div></div><div id=nav_top_icon style=display:none></div><div id=list_note_icon></div><script>$(function(){var scrollTop=$(window).scrollTop();if(scrollTop>0){$("#nav_top_icon").show(200);}
window.onscroll=function(){scrollTop=$(window).scrollTop();if(scrollTop==0){$("#nav_top_icon").hide(200);}else{$("#nav_top_icon").show(200);}}
$("#nav_top_icon").click(function(){$("body,html").animate({scrollTop:0},500);});$("#list_note_icon").click(function(){$("#popOuterDiv").toggle();$("#popDiv").slideToggle();$("#popCtn").html($("#TableOfContents").html());})
$("#popOuterDiv").click(function(){$("#popOuterDiv").hide();$("#popDiv").slideUp();})});document.addEventListener('DOMContentLoaded',()=>{hljs.highlightAll();hljs.initLineNumbersOnLoad({singleLine:true});changeSidebarHeight();switchDocToc();$("img").css("cursor:pointer")
$("img").click(function(){var _this=$(this);imgShow("#outerdiv","#innerdiv","#bigimg",_this);});$("ol").each(function(index){if(/\d{1,4}/.test($(this).attr("start"))){let idx=parseInt($(this).attr("start"));let lis=$(this).find("li");if(lis.length>0){lis.each(function(index){$(this).attr("id","step"+(idx+index));});}else{$(this).attr("id","step"+idx);}}});});</script><script src=/static/js/clipboard.min.js></script><script src=/static/js/codecopy.bundle.js?10></script></div></div></main><footer class=pl-scrollbar><div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text"></p></div></div></footer></body></html>
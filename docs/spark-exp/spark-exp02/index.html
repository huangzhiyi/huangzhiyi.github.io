<!doctype html><html lang=zh><meta charset=utf-8><meta name=viewport content="width=device-width"><title>第二章 搭建 Spark 实验手册 | 大数据分析和内存计算Spark | Heis</title><meta name=generator content="Hugo Eureka 0.8.4"><link rel=stylesheet href=/css/eureka.min.css><script defer src=/js/eureka.min.js></script><script src=/static/js/jquery-3.6.0.min.js></script><script src=/static/js/heis.js?230321></script><link rel=stylesheet href=/static/css/heis.css?230321 media=all onload="this.media='all';this.onload=null" crossorigin><link rel=stylesheet href=/static/css/print.css?230413 media=print crossorigin><link rel=stylesheet href=/static/css/highlight-css/solarized-light.min.css media=print onload="this.media='all';this.onload=null" crossorigin><script defer src=/static/js/highlight.min.js crossorigin></script><script defer src=/static/js/highlight-langs/python.min.js crossorigin></script><script defer src=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js integrity="sha256-uNYoXefWRqv+PsIF/OflNmwtKM4lStn9yrz2gVl6ymo=" crossorigin></script><script defer src=/static/js/highlightjs-line-numbers.min.js></script><link rel=icon type=image/png sizes=32x32 href=/images/icon_huddfe7d1590fd89155f46b33625956ea5_4357_32x32_fill_box_center_2.png><link rel=apple-touch-icon sizes=180x180 href=/images/icon_huddfe7d1590fd89155f46b33625956ea5_4357_180x180_fill_box_center_2.png><meta name=description content="Spark 第二章搭建 Spark 实验手册"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"系列文章","item":"/docs/"},{"@type":"ListItem","position":2,"name":"大数据分析和内存计算Spark","item":"/docs/spark-exp/"},{"@type":"ListItem","position":3,"name":"第二章 搭建 Spark 实验手册","item":"/docs/spark-exp/spark-exp02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"/docs/spark-exp/spark-exp02/"},"headline":"第二章 搭建 Spark 实验手册 | 大数据分析和内存计算Spark | Heis","datePublished":"2020-02-10T10:42:51+08:00","dateModified":"2020-02-10T10:42:51+08:00","wordCount":2870,"author":{"@type":"Person","name":"heis"},"publisher":{"@type":"Person","name":"C. Wang","logo":{"@type":"ImageObject","url":"/images/icon.png"}},"description":"Spark 第二章搭建 Spark 实验手册"}</script><meta property="og:title" content="第二章 搭建 Spark 实验手册 | 大数据分析和内存计算Spark | Heis"><meta property="og:type" content="article"><meta property="og:image" content="/images/icon.png"><meta property="og:url" content="/docs/spark-exp/spark-exp02/"><meta property="og:description" content="Spark 第二章搭建 Spark 实验手册"><meta property="og:locale" content="zh"><meta property="og:site_name" content="Heis"><meta property="article:published_time" content="2020-02-10T10:42:51+08:00"><meta property="article:modified_time" content="2020-02-10T10:42:51+08:00"><meta property="article:section" content="docs"><meta property="article:tag" content="spark"><meta property="article:tag" content="搭建"><script>var _hmt=_hmt||[];(function(){var hm=document.createElement("script");hm.src="https://hm.baidu.com/hm.js?f58b6a2675cf52000232edb4d109eccc";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script><body class="flex flex-col min-h-screen"><header id=headerctn class="fixed flex items-center w-full pl-scrollbar z-50 bg-secondary-bg shadow-sm"><div class="w-full max-w-screen-xl mx-auto"><script>let storageColorScheme=localStorage.getItem("lightDarkMode")
if(((storageColorScheme=='Auto'||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches)||storageColorScheme=="Dark"){document.getElementsByTagName('html')[0].classList.add('dark')}</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="mr-6 text-primary-text text-xl font-bold">Heis</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent mr-4">文章</a>
<a href=/docs/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item mr-4">系列</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>浅色</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>深色</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>自动</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById('lightDarkMode')
if(storageColorScheme==null||storageColorScheme=='Auto'){document.addEventListener('DOMContentLoaded',()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change',switchDarkMode)})}else if(storageColorScheme=="Light"){element.firstElementChild.classList.remove('fa-adjust')
element.firstElementChild.setAttribute("data-icon",'sun')
element.firstElementChild.classList.add('fa-sun')}else if(storageColorScheme=="Dark"){element.firstElementChild.classList.remove('fa-adjust')
element.firstElementChild.setAttribute("data-icon",'moon')
element.firstElementChild.classList.add('fa-moon')}
document.addEventListener('DOMContentLoaded',()=>{getcolorscheme();switchBurger();});</script></div></header><main class="flex-grow pt-16"><div class=pl-scrollbar><div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto"><div id=doc-container class=lg:pt-12><div class="flex flex-col md:flex-row bg-secondary-bg rounded"><div class="md:w-1/4 lg:w-1/5 border-r"><div class="sticky top-16 pt-6"><div id=sidebar-title class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text"><span class=font-semibold>目录</span>
<i class="fas fa-caret-right ml-1"></i></div><div id=sidebar-toc class="hidden md:block overflow-y-auto mx-6 md:mx-0 pr-6 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent"><div class="flex flex-wrap ml-4 -mr-2 p-2 bg-secondary-bg md:bg-primary-bg rounded"><a class=hover:text-eureka href=/docs/spark-exp/>大数据分析和内存计算Spark</a></div><ul class=pl-6><li class=py-2><div><a class="text-eureka hover:text-eureka" href=/docs/spark-exp/spark-exp02/>第二章 搭建 Spark 实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp03/>第三章 使用 Python 开发 Spark 应用实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp04/>第四章 Spark RDD 实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp05/>第五章 SparkSQL 实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp06/>第六章 Spark Streaming 实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp07/>第七章 Spark 机器学习库实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp08/>Spark 第八章 GraphFrames 图计算实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-training1/>Spark 综合实验1</a></div></li></ul></div></div></div><div class="w-full md:w-3/4 lg:w-4/5 pb-8 pt-2 md:pt-8"><div id=doc-tit class="w-full lg:w-3/4 pl-6 ml-0 mr-auto"><h1 class="font-bold text-3xl text-primary-text">第二章 搭建 Spark 实验手册</h1><div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text"><div class="mr-6 my-2"><i class="fas fa-calendar mr-1"></i><span>2020-02-10</span></div><div class="mr-6 my-2"><i class="fas fa-clock mr-1"></i><span>6分钟阅读时长</span></div><div class="mr-6 my-2"><i class="fas fa-folder mr-1"></i><a href=/categories/spark/ class=hover:text-eureka>spark</a></div></div></div><div class=flex><div class="w-full lg:w-3/4 px-6"><div class=content><h2 id=版本>【版本】</h2><p>当前版本号<code>v20200318</code></p><div class=tbl-start></div><table><thead><tr><th>版本</th><th>修改说明</th></tr></thead><tbody><tr><td>v20200318</td><td>修改了第16步脚本路径</td></tr><tr><td>v20200229</td><td>增加了hosts的设置</td></tr><tr><td>v20200210</td><td>初始化版本</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h2 id=实验21搭建-spark-standalone-模式>实验2.1：搭建 Spark Standalone 模式</h2><h3 id=实验目的>【实验目的】</h3><ul><li>掌握搭建 Spark Standalone 模式</li><li>熟练掌握Linux命令（vi、tar、环境变量修改等等）的使用</li><li>掌握VMWare、XShell、Xftp等客户端的使用</li></ul><h3 id=实验原理>【实验原理】</h3><p>Spark支持多种分布式部署方式，至少包括：</p><ul><li>Standalone单独部署（伪分布或全分布），不需要有依赖资源管理器。</li><li>Hadoop YARN（也即Spark On Yarn），部署到Hadoop YARN资源管理器。</li><li>Apache Mesos，部署到Apache Mesos资源管理器。</li><li>Amazon EC2，部署到Amazon EC2资源管理器。</li></ul><p>这里主要学习单独（Standalone）部署中的伪分布模式的搭建。</p><h3 id=实验环境>【实验环境】</h3><ul><li>内存：至少4G</li><li>硬盘：至少空余40G</li><li>操作系统: 64位 Windows系统。</li></ul><h3 id=实验资源>【实验资源】</h3><p>以下非网盘实验资源推荐复制链接到迅雷下载。</p><ul><li>虚拟机软件</li></ul><pre><code>https://pan.baidu.com/s/1CaDXO0GmJ2ocy7Gtuef47Q#提取码=fjks
</code></pre><ul><li>虚拟机镜像</li></ul><pre><code>https://pan.baidu.com/s/1g0Ji1wY07JxffhhHo14cIQ#提取码2txb
</code></pre><ul><li>XShell</li></ul><pre><code>https://pan.baidu.com/s/1e8isFVpfw90sps2WtxLZOQ#提取码=jef5
</code></pre><ul><li>scala-2.12.10.tgz</li></ul><pre><code>http://distfiles.macports.org/scala2.12/scala-2.12.10.tgz
</code></pre><ul><li>spark-2.4.4-bin-hadoop2.7.tgz</li></ul><pre><code>https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
</code></pre><h3 id=实验步骤>【实验步骤】</h3><ol><li><p>安装 VMWare WorkStation、XShell 如果已经安装请忽略。
安装过程略。</p></li><li><p>导入老师提供的 Node0 虚拟机镜像。
打开虚拟机->找到 Node0 文件夹下的 vmx 文件进行 导入。
注意：请分配给你的虚拟机至少2G内存。</p></li><li><p>右键点击虚拟机 Node0，选择“设置..”。配置网络适配器到 VMnet1 。
<img src=/static/img/c909e1e1-a6a6-47a0-9278-a6396d011e9a.png alt></p></li><li><p>进入电脑“控制面板\网络和 Internet\网络连接”，查看VMWare 的VMnet1虚拟网卡的属性，并配置 IP 为<code>192.168.30.1</code>。
<img src=/static/img/519bc1a7-c6e7-4298-be8e-883ea7d80f60.png alt></p></li></ol><p><img src=/static/img/d3e91a02-e5d3-4d9e-9d18-cc47acbe01ec.jpg alt></p><ol start=5><li><p>选择能够上网的网卡（无线上网选择无线网卡，有线上网则选择有线网卡），让上网的网卡共享给 VMWare 的虚拟网卡VMnet1。
<img src=/static/img/9dbe895f-4852-4b98-9557-19b1f3430e1f.jpg alt></p></li><li><p>启动 Node0 镜像，使用 XShell SSH 登录到 Node0。</p></li></ol><pre><code>用户名：hadoop
密码：Hdp0668
IP：192.168.30.130
</code></pre><ol start=7><li>进入网卡设置，检查IP，网关和网卡设置是否一致。</li></ol><p>(1)查看网卡名称和IP地址</p><pre><code class=language-bash>ip addr
</code></pre><pre><code>2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:49:ea:f6 brd ff:ff:ff:ff:ff:ff
    inet 192.168.30.130/24 brd 192.168.30.255 scope global ens33

</code></pre><p>(2)编辑网卡设置，注意其中的<code>ens33</code>是网卡名称和上文网卡名称要保持一致。</p><pre><code>sudo vim /etc/network/interfaces
</code></pre><pre><code>auto ens33
#iface ens33 inet dhcp
iface ens33 inet static

address 192.168.30.130

netmask 255.255.255.0

gateway 192.168.30.1
</code></pre><ol start=8><li>运行以下命令，检查网络是否畅通。如果没有回应则检查你之前的网络配置步骤是否出错。</li></ol><pre><code class=language-bash>ping  192.168.30.1
ping baidu.com
</code></pre><ol start=9><li>配置免密登录</li></ol><pre><code>ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
chmod 700 ~/.ssh
chmod 600 ~/.ssh/authorized_keys
</code></pre><ol start=10><li>检查环境。
（1）分别逐行运行以下命令查看预装的 Python3 和 Java 版本和安装路径。</li></ol><pre><code>#查看 JDK 版本
java -version
#查找 java 命令所在路径
whereis java
#查看 Python 版本
python -V
#查找 python3 命令所在路径
whereis python3
</code></pre><p>（2）把python3命令拷贝为python命令</p><pre><code>sudo cp /usr/bin/python3 /usr/bin/python
</code></pre><p>（3）修改hosts，指向hostname。</p><pre><code>sudo vim /etc/hosts
</code></pre><pre><code>127.0.0.1       node0
</code></pre><p>（4）另外系统也预装了MySQL，并在/opt目录下预装了Hadoop 伪分布式、Hive、Pig。</p><pre><code>#Hadoop 的启动和停止脚本修改为
start-hdp.sh
stop-hdp.sh

#启动Hive
hive

#启动Pig
pig

#查看Hadoop相关进程
jps
</code></pre><ol start=11><li>系统预装软件的环境变量的配置放在了<code>/etc/profile.d/set-env.sh</code>，需要提升权限编辑。</li></ol><pre><code class=language-bash>sudo vim /etc/profile.d/set-env.sh
</code></pre><ol start=12><li><p>上传 scala 和 spark 的安装包。</p></li><li><p>使用<code>tar -xvf</code>解压 scala 和 spark 包，并移动到/opt目录下。例如以下的目录结构。</p></li></ol><pre><code class=language-bash>/移动 Scala 和 Spark 目录到 /opt
sudo mv scala-2.12.10 /opt/scala-2-12

sudo mv spark-2.4.4-bin-hadoop2.7 /opt/spark
</code></pre><ol start=14><li>编辑环境变量</li></ol><p>（1）加入 <code>SCALA_HOME</code> 和 <code>SPARK_HOME</code></p><pre><code class=language-bash>SCALA_HOME=/opt/scala-2-12
SPARK_HOME=/opt/spark
export PYSPARK_PYTHON=python3
PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$HIVE_HOME/bin:$PIG_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$SCALA_HOME/bin:$PATH:.
</code></pre><p>（2）让环境变量生效</p><pre><code class=language-bash>source  /etc/profile.d/set-env.sh
</code></pre><ol start=15><li>配置 Spark</li></ol><p>（1）编辑<code>spark-env.sh</code></p><pre><code class=language-bash>cd $SPARK_HOME/conf
cp spark-env.sh.template spark-env.sh
vim spark-env.sh
</code></pre><pre><code class=language-bash>#spark-env.sh 配置
#Spark executor 可以使用的CPU核心数量，可以根据自己电脑和虚拟机情况来配置。
SPARK_EXECUTOR_CORES=2

#Spark master 绑定hostname
SPARK_MASTER_HOST=node0

#Spark worker 可以使用的CPU核心数量，可以根据自己电脑和虚拟机情况来配置。
SPARK_WORKER_CORES=2

#Spark worker 最大内存分配量
SPARK_WORKER_MEMORY=2g

#在文件最后加入以下配置，PySpark 的 Python 命令指定为 python3
alias python=python3
export JAVA_HOME=/opt/jdk8
export SCALA_HOME=/opt/scala-2-12
export PYSPARK_DRIVER_PYTHON=python3
export PYSPARK_PYTHON=python3
export LD_LIBRARY_PATH=/opt/hadoop/lib/native/
</code></pre><p>（2）编辑 slaves</p><pre><code>cp $SPARK_HOME/conf/slaves.template $SPARK_HOME/conf/slaves
</code></pre><pre><code>#底部加入本机hostname
node0
</code></pre><p>（3）复制日志配置</p><pre><code>cd $SPARK_HOME/conf
cp log4j.properties.template log4j.properties
</code></pre><ol start=16><li>修改 Spark 启动脚本名称。</li></ol><p>（1）在Spark启动脚本加入你的个人学号。</p><pre><code>vim $SPARK_HOME/sbin/start-all.sh

#在第二行加入以下内容，注意替换为你的学号
echo &quot;Start by 你的学号&quot;
</code></pre><p>（2）修改启动脚本的名称。</p><pre><code>cd $SPARK_HOME/sbin
mv start-all.sh start-spark.sh
mv stop-all.sh stop-spark.sh
</code></pre><ol start=17><li>离线安装PySpark。</li></ol><pre><code class=language-bash>cd $SPARK_HOME/python
sudo python3 setup.py install
</code></pre><ol start=18><li>启动 Spark，并验证。</li></ol><p>（1）输入启动脚本，并输入 jps 查看是否有 Worker 和 Master 进程</p><pre><code>start-spark.sh
jps
</code></pre><p>（2）使用本机电脑浏览器访问<code>http://虚拟机IP地址:8080</code>，查看 Spark 工作网页是否正常。</p><p>（3）运行<code>pyspark --master spark://node0:7077</code> 查看 PySpark 是否安装成功。（可以使用<code>quit()</code>命令退出pyspark）</p><hr><h2 id=实验22使用-spark-submit-命令>实验2.2：使用 spark submit 命令</h2><h2 id=实验名称>【实验名称】</h2><p>实验2.2：使用 spark submit 命令</p><h3 id=实验目的-1>【实验目的】</h3><ul><li>掌握 spark submit 命令</li><li>了解蒙特卡罗（Monte Carlo）方法计算圆周率的方法
正方形内部有一个相切的圆，假设圆的半径为R，则圆形和正方形面积分别是πR<sup>2</sup>和4R<sup>2</sup>，它们面积之比是π/4。如果使用大量点随机填充到正方形内，那么只需要计算圆内的点（即点到圆心距离小于圆半径）占到所有点的比例，即可推导出 π 的近似值。</li></ul><p><img src=/static/img/MonteCarlo.png alt></p><h3 id=实验原理-1>【实验原理】</h3><ul><li><p><code>spark-submit -h</code>可以查看用法</p></li><li><p>Spark Master URL参数解析</p></li></ul><div class=tbl-start></div><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>local</td><td>使用一个 Worker 线程本地化运行 Spark（默认）</td></tr><tr><td>local[k]</td><td>使用 k 个 Worker 线程本地化运行 Spark</td></tr><tr><td>local[*]</td><td>使用 k 个 Worker 线程本地化运行 Spark（这里 K 自动设置为机器的 CPU 核数）</td></tr><tr><td>spark://HOST:PORT</td><td>连接到指定的 Spark 单机版集群 master。必须使用 master 所配置的接口，默认接口 7077，如 spark://10.10.10.10:7077。</td></tr><tr><td>mesos://HOST:PORT</td><td>连接到指定的 mesos 集群。host 参数是 moses master 的 hostname。必须使用 master 所配置的接口，默认接口是 5050。</td></tr><tr><td>yarn-client</td><td>以客户端模式连接到 yarn 集群，集群位置由环境变量 HADOOP_CONF_DIR 决定。</td></tr><tr><td>yarn-cluster</td><td>以集群模式连接到 yarn 集群，同样由 HADOOP_CONF_DIR 决定连接。</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h3 id=实验环境-1>【实验环境】</h3><ul><li>Ubuntu 16.04</li><li>Spark2.x</li></ul><h3 id=实验步骤-1>【实验步骤】</h3><ol><li>输入<code>spark-submit -h </code>查看各参数的定义</li></ol><pre><code>spark-submit -h
</code></pre><ol start=2><li>执行下面几种 spark-submit 方式。注意：在运行后注意打开网页http://[IP地址]:8080，查看是否有应用程序（Application）。</li></ol><p>（1）方式1：没有指定“&ndash;master”</p><pre><code>spark-submit --class org.apache.spark.examples.SparkPi  /opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar  100
</code></pre><blockquote><p>注：这里的100作为参数传入 SparkPi 类，是指把计算集分区数量。</p></blockquote><p>（2）方式2：指定“&ndash;master”为local</p><pre><code>spark-submit --master local  --class org.apache.spark.examples.SparkPi  /opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar  100
</code></pre><p>（3）方式3：指定“&ndash;master”为local[2]</p><pre><code>spark-submit --master local[2]  --class org.apache.spark.examples.SparkPi  /opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar  100
</code></pre><p>（4）方式4：指定“&ndash;master”为local[*]</p><pre><code>spark-submit --master local[*]  --class org.apache.spark.examples.SparkPi  /opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar  100
</code></pre><p>（5）方式5：指定“&ndash;master”为spark://hostname:7077</p><p>先启动 Spark</p><pre><code>start-spark.sh
</code></pre><pre><code>spark-submit --master spark://node0:7077  --class org.apache.spark.examples.JavaSparkPi  /opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar 100
</code></pre><p>（6）方式6：运行py文件</p><pre><code>spark-submit --master spark://node0:7077  /opt/spark/examples/src/main/python/pi.py   100
</code></pre><blockquote><p>注:以下为pi.py的源码</p></blockquote><pre><code class=language-python>import sys
from random import random
from operator import add

from pyspark import SparkContext


if __name__ == &quot;__main__&quot;:
    &quot;&quot;&quot;
        Usage: pi [partitions]
    &quot;&quot;&quot;
    sc = SparkContext(appName=&quot;PythonPi&quot;)
	#传入的参数作为分区数量
    partitions = int(sys.argv[1]) if len(sys.argv) &gt; 1 else 2
	#点的数量,每个分区10万个点
    n = 100000 * partitions

    #如果到圆心距离小于1（即圆半径），返回1，表示在圆面积以内
    def f(_):
        x = random() * 2 - 1
        y = random() * 2 - 1
        return 1 if x ** 2 + y ** 2 &lt; 1 else 0

	#累计圆面积内的点数
    count = sc.parallelize(xrange(1, n + 1), partitions).map(f).reduce(add)
	#推导Pi的近似值
    print &quot;Pi is roughly %f&quot; % (4.0 * count / n)

    sc.stop()
</code></pre></div><div class=my-4><a href=/tags/spark/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#spark</a>
<a href=/tags/%E6%90%AD%E5%BB%BA/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#搭建</a></div><div class=py-2><div class="flex flex-col md:flex-row items-center my-8"><a href=/authors/heis/ class="w-24 h-24 md:mr-4"><img src=/static/img/authors/heis.png class="w-full bg-primary-bg rounded-full" alt=Avatar></a><div class="w-full md:w-auto mt-4 md:mt-0"><a href=/authors/heis/ class="block font-bold text-lg pb-1 mb-2 border-b">黄老师</a>
<span class="block pb-2"></span><a href=mailto:heishuangzy@qq.com class=mr-1><i class="fas fa-envelope"></i></a><a href=https://gitee.com/heis/ class=mr-1><i class="fab fa-git"></i></a><a href=http://heis.gitee.io/ class=mr-1><i class="fas fa-blog"></i></a></div></div></div><div id=btmQrcode></div><p style=text-align:center>扫码或长按识别访问</p><script src=/static/js/qrcode.min.js></script><script type=text/javascript>function getURL(){var url=window.location.href;if(window.location.hash!=""){url=url.substring(0,url.indexOf("#"));}
return url;}
new QRCode(document.getElementById("btmQrcode"),{text:getURL(),width:192,height:192,colorDark:"#000000",colorLight:"#ffffff",correctLevel:QRCode.CorrectLevel.H});</script><div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t"><div id=presec></div><div id=nextsec class="md:text-right mt-4 md:mt-0"><span class="block font-bold">下一页</span>
<a href=/docs/spark-exp/spark-exp03/ class=block>第三章 使用 Python 开发 Spark 应用实验手册</a></div></div></div><div class="hidden lg:block lg:w-1/4"><div class="sticky top-16 z-10 hidden lg:block px-6 py-4 bg-secondary-bg pt-16 -mt-16"><span class="text-lg font-semibold">本页内容</span></div><div class="sticky-toc hidden lg:block px-6 pb-6 pt-10 -mt-10 border-l"><nav id=TableOfContents><ul><li><a href=#版本>【版本】</a></li><li><a href=#实验21搭建-spark-standalone-模式>实验2.1：搭建 Spark Standalone 模式</a><ul><li><a href=#实验目的>【实验目的】</a></li><li><a href=#实验原理>【实验原理】</a></li><li><a href=#实验环境>【实验环境】</a></li><li><a href=#实验资源>【实验资源】</a></li><li><a href=#实验步骤>【实验步骤】</a></li></ul></li><li><a href=#实验22使用-spark-submit-命令>实验2.2：使用 spark submit 命令</a></li><li><a href=#实验名称>【实验名称】</a><ul><li><a href=#实验目的-1>【实验目的】</a></li><li><a href=#实验原理-1>【实验原理】</a></li><li><a href=#实验环境-1>【实验环境】</a></li><li><a href=#实验步骤-1>【实验步骤】</a></li></ul></li></ul></nav></div><script>window.addEventListener('DOMContentLoaded',()=>{enableStickyToc();});</script></div></div></div></div></div><div id=outerdiv><div id=innerdiv><img id=bigimg src></div></div><div id=popOuterDiv><div id=popDiv style=display:none><div class=close><span onclick="$('#popDiv').hide();"></span></div><p id=popCtn></p></div></div><div id=nav_top_icon style=display:none></div><div id=list_note_icon></div><script>$(function(){var scrollTop=$(window).scrollTop();if(scrollTop>0){$("#nav_top_icon").show(200);}
window.onscroll=function(){scrollTop=$(window).scrollTop();if(scrollTop==0){$("#nav_top_icon").hide(200);}else{$("#nav_top_icon").show(200);}}
$("#nav_top_icon").click(function(){$("body,html").animate({scrollTop:0},500);});$("#list_note_icon").click(function(){$("#popOuterDiv").toggle();$("#popDiv").slideToggle();$("#popCtn").html($("#TableOfContents").html());})
$("#popOuterDiv").click(function(){$("#popOuterDiv").hide();$("#popDiv").slideUp();})});document.addEventListener('DOMContentLoaded',()=>{hljs.highlightAll();hljs.initLineNumbersOnLoad({singleLine:true});changeSidebarHeight();switchDocToc();$("img").css("cursor:pointer")
$("img").click(function(){var _this=$(this);imgShow("#outerdiv","#innerdiv","#bigimg",_this);});$("ol").each(function(index){if(/\d{1,4}/.test($(this).attr("start"))){let idx=parseInt($(this).attr("start"));let lis=$(this).find("li");if(lis.length>0){lis.each(function(index){$(this).attr("id","step"+(idx+index));});}else{$(this).attr("id","step"+idx);}}});});</script><script src=/static/js/clipboard.min.js></script><script src=/static/js/codecopy.bundle.js?247></script></div></div></main><footer class=pl-scrollbar><div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text"></p></div></div></footer></body></html>
<!doctype html><html lang=zh><meta charset=utf-8><meta name=viewport content="width=device-width"><title>第五章 SparkSQL 实验手册 | Spark 课程 | Heis</title><meta name=generator content="Hugo Eureka 0.8.4"><link rel=stylesheet href=/css/eureka.min.css><script defer src=/js/eureka.min.js></script><script src=/static/js/jquery-3.6.0.min.js></script><script src=/static/js/heis.js></script><link rel=stylesheet href=/static/css/heis.css media=all onload="this.media='all';this.onload=null" crossorigin><link rel=stylesheet href=/static/css/print.css media=print crossorigin><link rel=stylesheet href=/static/css/highlight-css/solarized-light.min.css media=print onload="this.media='all';this.onload=null" crossorigin><script defer src=/static/js/highlight.min.js crossorigin></script><script defer src=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js integrity="sha256-uNYoXefWRqv+PsIF/OflNmwtKM4lStn9yrz2gVl6ymo=" crossorigin></script><script defer src=/static/js/highlightjs-line-numbers.min.js></script><link rel=icon type=image/png sizes=32x32 href=/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_2.png><link rel=apple-touch-icon sizes=180x180 href=/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_2.png><meta name=description content="Spark 第五章 SparkSQL 实验手册"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"课程文档","item":"/docs/"},{"@type":"ListItem","position":2,"name":"Spark 课程","item":"/docs/spark-exp/"},{"@type":"ListItem","position":3,"name":"第五章 SparkSQL 实验手册","item":"/docs/spark-exp/spark-exp05/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"/docs/spark-exp/spark-exp05/"},"headline":"第五章 SparkSQL 实验手册 | Spark 课程 | Heis","datePublished":"2020-02-14T10:42:51+08:00","dateModified":"2020-02-14T10:42:51+08:00","wordCount":4639,"author":{"@type":"Person","name":"heis"},"publisher":{"@type":"Person","name":"C. Wang","logo":{"@type":"ImageObject","url":"/images/icon.png"}},"description":"Spark 第五章 SparkSQL 实验手册"}</script><meta property="og:title" content="第五章 SparkSQL 实验手册 | Spark 课程 | Heis"><meta property="og:type" content="article"><meta property="og:image" content="/images/icon.png"><meta property="og:url" content="/docs/spark-exp/spark-exp05/"><meta property="og:description" content="Spark 第五章 SparkSQL 实验手册"><meta property="og:locale" content="zh"><meta property="og:site_name" content="Heis"><meta property="article:published_time" content="2020-02-14T10:42:51+08:00"><meta property="article:modified_time" content="2020-02-14T10:42:51+08:00"><meta property="article:section" content="docs"><meta property="article:tag" content="spark"><meta property="article:tag" content="SQL"><meta property="article:tag" content="SparkSQL"><meta property="og:see_also" content="/docs/spark-exp/spark-exp03/"><meta property="og:see_also" content="/docs/spark-exp/spark-exp04/"><meta property="og:see_also" content="/docs/spark-exp/spark-exp02/"><script>var _hmt=_hmt||[];(function(){var hm=document.createElement("script");hm.src="https://hm.baidu.com/hm.js?f58b6a2675cf52000232edb4d109eccc";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script><body class="flex flex-col min-h-screen"><header id=headerctn class="fixed flex items-center w-full pl-scrollbar z-50 bg-secondary-bg shadow-sm"><div class="w-full max-w-screen-xl mx-auto"><script>let storageColorScheme=localStorage.getItem("lightDarkMode")
if(((storageColorScheme=='Auto'||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches)||storageColorScheme=="Dark"){document.getElementsByTagName('html')[0].classList.add('dark')}</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="mr-6 text-primary-text text-xl font-bold">Heis</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent mr-4">文章</a>
<a href=/docs/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item mr-4">课程文档</a>
<a href=/static/dm.html class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent mr-4">点名</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>浅色</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>深色</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>自动</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById('lightDarkMode')
if(storageColorScheme==null||storageColorScheme=='Auto'){document.addEventListener('DOMContentLoaded',()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change',switchDarkMode)})}else if(storageColorScheme=="Light"){element.firstElementChild.classList.remove('fa-adjust')
element.firstElementChild.setAttribute("data-icon",'sun')
element.firstElementChild.classList.add('fa-sun')}else if(storageColorScheme=="Dark"){element.firstElementChild.classList.remove('fa-adjust')
element.firstElementChild.setAttribute("data-icon",'moon')
element.firstElementChild.classList.add('fa-moon')}
document.addEventListener('DOMContentLoaded',()=>{getcolorscheme();switchBurger();});</script></div></header><main class=flex-grow><div class=pl-scrollbar><div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto"><div class=lg:pt-12><div class="flex flex-col md:flex-row bg-secondary-bg rounded"><div class="md:w-1/4 lg:w-1/5 border-r"><div class="sticky top-16 pt-6"><div id=sidebar-title class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text"><span class=font-semibold>目录</span>
<i class="fas fa-caret-right ml-1"></i></div><div id=sidebar-toc class="hidden md:block overflow-y-auto mx-6 md:mx-0 pr-6 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent"><div class="flex flex-wrap ml-4 -mr-2 p-2 bg-secondary-bg md:bg-primary-bg rounded"><a class=hover:text-eureka href=/docs/spark-exp/>Spark 课程</a></div><ul class=pl-6><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp02/>第二章 搭建 Spark 实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp03/>第三章 使用 Python 开发 Spark 应用实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp04/>第四章 Spark RDD 实验手册</a></div></li><li class=py-2><div><a class="text-eureka hover:text-eureka" href=/docs/spark-exp/spark-exp05/>第五章 SparkSQL 实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp06/>第六章 Spark Streaming 实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp07/>第七章 Spark 机器学习库实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-exp08/>Spark 第八章 GraphFrames 图计算实验手册</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/spark-exp/spark-training1/>Spark 综合实验1</a></div></li></ul></div></div></div><div class="w-full md:w-3/4 lg:w-4/5 pb-8 pt-2 md:pt-8"><div class="w-full lg:w-3/4 pl-6 ml-0 mr-auto"><h1 class="font-bold text-3xl text-primary-text">第五章 SparkSQL 实验手册</h1><div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text"><div class="mr-6 my-2"><i class="fas fa-calendar mr-1"></i><span>2020-02-14</span></div><div class="mr-6 my-2"><i class="fas fa-clock mr-1"></i><span>10分钟阅读时长</span></div><div class="mr-6 my-2"><i class="fas fa-folder mr-1"></i><a href=/categories/spark/ class=hover:text-eureka>spark</a></div></div></div><div class=flex><div class="w-full lg:w-3/4 px-6"><div class=content><h2 id=版本>【版本】</h2><p>当前版本号<code>v20200418</code></p><div class=tbl-start></div><table><thead><tr><th>版本</th><th>修改说明</th></tr></thead><tbody><tr><td>v20200418</td><td>修改 hosts 的说明，增加常见问题解答</td></tr><tr><td>v20200408</td><td>加入hosts的检查</td></tr><tr><td>v20200407</td><td>修正实验5.3一处描述错误</td></tr><tr><td>v20200406</td><td>修复实验5.3复制命令为cp</td></tr><tr><td>v20200322</td><td>初始化版本</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h2 id=实验名称-实验51rdd-dataframe-api-练习>【实验名称】 实验5.1：RDD DataFrame API 练习</h2><h3 id=实验目的>【实验目的】</h3><p>掌握RDD DataFrame 的相关 API</p><h3 id=实验原理>【实验原理】</h3><p>DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维
表 格。 DataFrame 与 RDD 的 主 要区 别在 于， 前 者带 有 schema 元信 息 ，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。采用RDD的toDF()算子将RDD转为DataFrame。</p><p>rdd 转换 DataFrame API：</p><pre><code>rdd.toDF(schema=None, sampleRatio=None)
</code></pre><ul><li>Schema：指表的结构信息</li><li>samplingRatio：指定用于类型推断的样本的比例。</li></ul><p>SparkSession 创建 DataFrame</p><pre><code>SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)
</code></pre><ul><li>schema ：指定表结构。</li><li>samplingRatio：指定用于类型推断的样本的比例。
含义是：如果df的某列的类型不确定，则抽样百分之samplingRatio的数据来看是什么类型。</li><li>verifySchema：验证每行的数据类型是否符合schema的定义。</li></ul><h3 id=实验环境>【实验环境】</h3><ul><li>操作系统：Ubuntu 16.04</li><li>Spark：Spark 2.x</li></ul><h3 id=实验要求>【实验要求】</h3><ol><li>请补充以下代码，按照要求输入结果。注意替换你的学号和姓名。</li></ol><pre><code>rdd1=sc.parallelize([('Michael','329'),('Andy','330'),('你的姓名','你的学号后三位')])
&lt;补充代码&gt;
</code></pre><p>期望输出：</p><pre><code>+--------+------+
|stuname |stuid |
+--------+------+
|Michael | 329  |
|   Andy | 330  |
|你的姓名| 你的学号后三位|
+--------+------+
</code></pre><ol start=2><li>使用 createDataFrame 函数创建以下内容并输出。注意替换你的学号和姓名</li></ol><pre><code>from pyspark.sql import Row
&lt;补充代码&gt;
</code></pre><p>期望输出：</p><pre><code>[Row(stuname='Michael', stuid='329'),
 Row(stuname='Andy', stuid='330'),
 Row(stuname='你的姓名', stuid='你的学号后三位')]
</code></pre><ol start=3><li>请补充以下代码，按照要求输入结果。</li></ol><pre><code>from pyspark.sql.types import *
rdd1=sc.parallelize([('Michael',29,73.5)])
schema = StructType(&lt;补充代码&gt;)
df = rdd1.toDF(schema)
df.printSchema()
</code></pre><p>期望输出：</p><pre><code>root
 |-- name: string (nullable = false)
 |-- age: integer (nullable = true)
 |-- weight: float (nullable = true)
</code></pre><ol start=4><li>请补充以下代码，按照要求输入结果。</li></ol><pre><code>rdd = sc.parallelize( [{'name': 'Alice', 'age': 25}])
spark.createDataFrame(rdd, &quot;&lt;补充代码&gt;&quot;).printSchema()
</code></pre><p>期望输出：</p><pre><code>root
 |-- name: string (nullable = true)
 |-- age: integer (nullable = true)
</code></pre><ol start=5><li>请补充以下代码，按照要求输入结果。</li></ol><pre><code>rdd = sc.parallelize( [('Michael','329')])
spark.createDataFrame(rdd, &quot;name: string, age: int&quot;).&lt;补充代码&gt;
</code></pre><pre><code>[('name', 'string'), ('age', 'int')]
</code></pre><hr><h2 id=实验名称-实验52自行设计实现rdd转为dataframe>【实验名称】 实验5.2：自行设计实现RDD转为DataFrame</h2><h3 id=实验目的-1>【实验目的】</h3><p>掌握RDD转为DataFrame的方法</p><h3 id=实验原理-1>【实验原理】</h3><p>同上</p><h3 id=实验环境-1>【实验环境】</h3><ul><li>操作系统：Ubuntu 16.04</li><li>Spark：Spark2.x</li></ul><h3 id=实验资源>【实验资源】</h3><p>数据下载：</p><pre><code>https://pan.baidu.com/s/1kXZ-bMTMXA1A3rszHebMBA#提取码t7q1
</code></pre><h3 id=实验背景>【实验背景】</h3><p>emp.csv是职工表。相关字段名如下图所示：</p><div class=tbl-start></div><table><thead><tr><th>序号</th><th>名称</th><th>类型</th><th>Nullable</th></tr></thead><tbody><tr><td>1</td><td>EMPNO</td><td>integer</td><td>true</td></tr><tr><td>2</td><td>ENAME</td><td>string</td><td>true</td></tr><tr><td>3</td><td>JOB</td><td>string</td><td>true</td></tr><tr><td>4</td><td>MGR</td><td>string</td><td>true</td></tr><tr><td>5</td><td>HIREDATE</td><td>string</td><td>true</td></tr><tr><td>6</td><td>SAL</td><td>integer</td><td>true</td></tr><tr><td>7</td><td>COMM</td><td>integer(空白转为0)</td><td>true</td></tr><tr><td>8</td><td>DEPTNO</td><td>integer</td><td>true</td></tr></tbody></table><div class=tbl-end style=height:10px></div><p>dept.csv是部门表。相关字段名如下图所示：</p><div class=tbl-start></div><table><thead><tr><th>序号</th><th>名称</th><th>类型</th><th>Nullable</th></tr></thead><tbody><tr><td>1</td><td>DEPTNO</td><td>integer</td><td>true</td></tr><tr><td>2</td><td>DNAME</td><td>string</td><td>true</td></tr><tr><td>3</td><td>LOC</td><td>string</td><td>true</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h3 id=实验要求-1>【实验要求】</h3><p>请按要求完成以下实验内容，要求在实验报告记录你的思路，代码和结果。</p><ol><li><p>修改 emp.csv 的第一行数据 SMITH 为你的姓名。上传 emp.csv 和 dept.csv 文件到虚拟机。</p></li><li><p>基于外部数据源 emp.csv 创建 RDD，再采用 RDD.toDF()将它转为 DataFrame。要求：</p></li></ol><ul><li>（1）并使用 schema 为 StructType 的方式。</li><li>（2）输出DataFrame.collect()的内容。</li><li>（3）输出DataFrame.printSchema()的内容。</li></ul><ol start=3><li>基于外部数据源 dept.csv 创建RDD，再采用 spark 的 createDataFrame() 将它转为 DataFrame。要求:</li></ol><ul><li>（1）schema 为"fieldName:fieldType"的方式。</li><li>（2）输出DataFrame.collect()的内容。</li><li>（3）输出DataFrame.printSchema()的内容。</li></ul><hr><h2 id=实验名称-实验53部署-spark-访问-hive-数据>【实验名称】 实验5.3：部署 Spark 访问 Hive 数据</h2><h3 id=实验目的-2>【实验目的】</h3><p>掌握Spark 访问 Hive 数据的部署方法</p><h3 id=实验环境-2>【实验环境】</h3><ul><li>操作系统：Ubuntu 16.04</li><li>Spark 2.x</li><li>Hive 2.x</li><li>Hadoop 2.7.3</li><li>PyCharm 或 Jupyter Notebook</li></ul><h3 id=实验资源-1>【实验资源】</h3><p>数据下载：</p><pre><code>https://pan.baidu.com/s/1kXZ-bMTMXA1A3rszHebMBA#提取码t7q1
</code></pre><h3 id=实验原理-2>【实验原理】</h3><p>通过配置 Spark，让 Spark 可以访问 Hive 的数据和使用 SparkSession.sql() 函数查询 DataFrame 数据。</p><p>通过把 Hadoop 的 core-site.xml 和 hdfs-site.xml，和 Hive 的配置文件 hive-site.xml 复制到 SPARK_HOME/conf 目录下。</p><h3 id=hive优化参数配置和解决常见问题>【Hive优化参数配置和解决常见问题】</h3><ol><li>解决<code>Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</code>问题。</li></ol><p>这通常是由于 Hive 的 MetaStore Server校验 Schema 出错导致的。只需要关闭校验选项即可。找到Hive安装目录/conf/hive-site.xml，增加以下选项配置。</p><pre><code>&lt;property&gt;
     &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;
     &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
</code></pre><ol start=2><li>优化 Hive 执行卡顿的参数配置。启动 Hive 以后，可以在 Hive 终端执行以下参数设置，优化运行效率。</li></ol><pre><code class=language-shell>#100%执行完 map 再执行 reduce，避免运行 map 同时又运行 reduce，占用太多资源。
hive&gt;set mapreduce.job.reduce.slowstart.completedmaps=1.0;

# 设置 mapreduce 执行内存为 2048MB，可视虚拟机资源情况自行调整。
hive&gt;set mapreduce.map.memory.mb=2048;

# 设置 mapreduce CPU 执行核数为2，可视虚拟机资源情况自行调整。
hive&gt;set mapreduce.map.cpu.vcores=2;
</code></pre><ol start=3><li>使用 Hive 日志进行错误诊断。</li></ol><p>Hive 的日志默认是在<code>/tmp/{用户名}/hive.log</code>。如果是使用Node0虚拟机镜像的Hadoop用户，则是在<code>/tmp/hadoop/hive.log</code>。</p><p>如果需要调整 hive 的日志路径，可以在<code>Hive安装目录/conf/</code>下修改日志配置。</p><pre><code class=language-shell>cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties

vim hive-exec-log4j2.properties
</code></pre><p>修改<code>property.hive.log.dir</code>选项</p><pre><code>#property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
property.hive.log.dir = /opt/hive/logs
</code></pre><h3 id=实验步骤>【实验步骤】</h3><ol><li>把 Hadoop 的 core-site.xml 和 hdfs-site.xml，和 Hive 的配置文件 hive-site.xml 复制到 SPARK_HOME/conf 目录下。</li></ol><pre><code>cp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf
cp $HADOOP_HOME/etc/hadoop/core-site.xml $SPARK_HOME/conf
cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml $SPARK_HOME/conf
</code></pre><ol start=2><li>打开 SPARK_HOME/conf 目录，查看三个配置文件是否复制成功。</li></ol><pre><code>ls $SPARK_HOME/conf
</code></pre><p>注意，如果<code>hive-site.xml</code>内配置的 MySQL 链接如果是<code>jdbc:mysql://localhost:3306/hive?useSSL=false</code>，则要确保/etc/hosts下配置 localhost 指向 127.0.0.1。</p><pre><code>sudo vim /etc/hosts

#确保有以下2条，没有则补充
127.0.0.1 localhost
127.0.0.1 node0
</code></pre><ol start=3><li>请从以下链接下载<code>dept.csv</code>，并上传到HDFS 根目录下。过程略。</li></ol><pre><code>https://pan.baidu.com/s/1kXZ-bMTMXA1A3rszHebMBA #提取码t7q1
</code></pre><ol start=4><li>启动 hadoop 和 hive。</li></ol><pre><code>start-hdp.sh
hive
</code></pre><ol start=5><li>使用 Hive 创建数据库</li></ol><pre><code>hive&gt; create database spark;
hive&gt; use spark;
</code></pre><ol start=6><li>导入<code>dept.csv</code>为表<code>dept001</code>。请替换001为你的学号后三位。</li></ol><pre><code>hive&gt; create table spark.dept001 (deptno INT, name string, location string) COMMENT 'dept001' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
hive&gt; load data inpath '/dept.csv' into table dept001;
</code></pre><ol start=7><li>使用 Hive 查询 <code>dept001</code>。请替换001为你的学号后三位。</li></ol><pre><code>hive&gt; select * from dept001;
</code></pre><ol start=8><li>退出<code>Hive</code>，启动<code>spark</code>和<code>pyspark</code>。</li></ol><pre><code>start-spark.sh
pyspark --master spark://node0:7077 --jars /opt/hive/lib/mysql-connector-java-5.1.48.jar
</code></pre><ol start=9><li>测试能够在 Spark 调用 Hive 的数据。请替换001为你的学号后三位。</li></ol><pre><code>spark.sql('select * from spark.dept001 limit 1').show()
</code></pre><hr><h2 id=实验名称-实验54使用dataframe分析出租车的-gps-信息>【实验名称】 实验5.4：使用DataFrame分析出租车的 GPS 信息</h2><h3 id=实验目的-3>【实验目的】</h3><p>掌握DataFrame的常用操作</p><h3 id=实验环境-3>【实验环境】</h3><ul><li>操作系统：Ubuntu 16.04</li><li>Spark2.x</li><li>PyCharm 或 Jupyter Notebook</li></ul><h3 id=实验原理-3>【实验原理】</h3><p>使用 Spark DataFrame 对 csv 数据进行查询</p><h3 id=实验资源-2>【实验资源】</h3><p>请从以下链接下载某地区出租车 GPS 定位数据<code>taxi.csv</code>。</p><pre><code>https://pan.baidu.com/s/1kXZ-bMTMXA1A3rszHebMBA#提取码t7q1
</code></pre><p>部分数据见下图：</p><div class=tbl-start></div><table><thead><tr><th>编号（id）</th><th>纬度（lat）</th><th>经度（lon）</th><th>时间戳（time）</th></tr></thead><tbody><tr><td>1</td><td>30.624806</td><td>104.136604</td><td>211846</td></tr><tr><td>1</td><td>30.624809</td><td>104.136612</td><td>211815</td></tr><tr><td>1</td><td>30.624811</td><td>104.136587</td><td>212017</td></tr><tr><td>1</td><td>30.624811</td><td>104.136596</td><td>211916</td></tr><tr><td>1</td><td>30.624811</td><td>104.136619</td><td>211744</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h3 id=实验要求-2>【实验要求】</h3><p>根据上述条件，结合课堂上学习的“DataFrame的常用操作”相关知识，编写代码实现如下要求，请把在实验报告记录你的思考过程，完整代码和实验结果。</p><ul><li>（1）查询编号为 5 的出租车的 GPS 数据的前 10 行。</li><li>（2）统计出租车的总数有多少。</li><li>（3）统计出租车id为297的 GPS 点记录有多少条。</li></ul><hr><h2 id=实验名称-实验55使用sparksql分析图书信息>【实验名称】 实验5.5：使用SparkSQL分析图书信息</h2><h3 id=实验目的-4>【实验目的】</h3><p>掌握SparkSQL的常用操作</p><h3 id=实验原理-4>【实验原理】</h3><p>请从以下链接下载图书数据book.txt</p><pre><code>https://pan.baidu.com/s/1kXZ-bMTMXA1A3rszHebMBA #提取码t7q1
</code></pre><p>数据内容举例如下：</p><div class=tbl-start></div><table><thead><tr><th>seq（序号）</th><th>name（书名）</th><th>rating（评分）</th><th>price（价格）</th><th>pub（出版社）</th><th>url</th></tr></thead><tbody><tr><td>40332</td><td>简爱</td><td>8.8</td><td>29.80元</td><td>上海世界图书出版公司</td><td><a href=https://book.douban.com/subject/>https://book.douban.com/subject/</a></td></tr></tbody></table><div class=tbl-end style=height:10px></div><h3 id=实验环境-4>【实验环境】</h3><ul><li>操作系统：Ubuntu 16.04</li><li>Spark2.x</li><li>PyCharm 或 Jupyter Notebook</li></ul><h3 id=实验要求-3>【实验要求】</h3><p>根据上述条件，结合课堂上学习的 Spark SQL 相关知识，编写代码实现如下要求，请把在实验报告记录你的思考过程，完整代码和实验结果。</p><ul><li><p>（1）创建RDD，并将RDD转为DataFrame。</p></li><li><p>（2）调用 createOrReplaceTempView，创建临时视图名为<code>book001</code>，注意替换001为你的学号后三位。</p></li><li><p>（3）使用 SQL 语句查询前10条数据。</p></li><li><p>（4）统计书名包含“微积分”的书的数量</p></li><li><p>（5）查询评分大于等于9分，小于等于9.5分，且书名包含“艺术”的书，只展示前10条的序号、书名和评分，按分数从高到低排列。</p></li><li><p>（6）计算所有书名包含“艺术”的评分平均值</p></li><li><p>（7）统计出版书数量最大的出版社前10名，按数量从高到低排列。</p></li></ul><hr><h2 id=实验名称-实验56dataframe的保存和加载>【实验名称】 实验5.6：DataFrame的保存和加载</h2><h3 id=实验目的-5>【实验目的】</h3><p>掌握DataFrame的保存和加载</p><h3 id=实验原理-5>【实验原理】</h3><p>parquet文件的加载示例：</p><pre><code>userDF = spark.read.load(&quot;/home/hadoop/users.parquet&quot;)
</code></pre><p>json文件的加载示例：</p><pre><code>peopleDF = spark.read.json(&quot;/home/hadoop/people.json&quot;)
peopleDF = spark.read.format(&quot;json&quot;).load(&quot;/home/hadoop/people.json&quot;)
</code></pre><p>保存到parquet文件示例：</p><pre><code>userDF.select(&quot;name&quot;,&quot;favorite_color&quot;).write.save(&quot;/home/hadoop/result1&quot;)
userDF.select(&quot;name&quot;,&quot;favorite_color&quot;).write.mode(&quot;overwrite&quot;).save(&quot;/home/hadoop/result1&quot;)
</code></pre><p>保存到csv文件示例：</p><pre><code>userDF.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;csv&quot;).save(&quot;/home/hadoop/result2&quot;)
userDF.select(&quot;name&quot;,&quot;favorite_color&quot;).write.csv(&quot;/home/hadoop/temp/result3&quot;)
</code></pre><p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.DataFrameReader" target=_blank>读取文件的相关文档>></a></p><p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.DataFrameWriter" target=_blank>写入文件的相关文档>></a></p><h3 id=实验环境-5>【实验环境】</h3><ul><li>操作系统：Ubuntu 16.04</li><li>Spark：Spark2.x</li><li>pyspark</li></ul><h3 id=实验步骤-1>【实验步骤】</h3><ol><li><p>统计每个出版社评分最高的书，展示书的序号，名称，评分和出版社。并且按照评分从高到低排列。保存结果到<code>/home/hadoop/pub-best001</code>目录。注意替换001为你的学号后三位。</p></li><li><p>加载步骤1生成的 CSV 文件，并且筛选书名含有“艺术”2字的，评分最高的10本书，按按照评分从高到低排列。</p></li></ol><blockquote><p>提示：从 CSV 读取数据需要重新配置 Schema。</p></blockquote><h2 id=常见问题解答faq>【常见问题解答FAQ】</h2><h3 id=问题1使用-spark-sql-的时候频繁遇到-py4j-的connection-error-或者-jvm-提示cannot-allocate-memory错误>问题1：使用 Spark SQL 的时候频繁遇到 Py4j 的<code>Connection error</code> 或者 JVM 提示<code>Cannot allocate memory</code>错误。</h3><p>答：这些问题一般都是由于虚拟机的内存不足导致的。可以使用<code>free -m</code>命令查看虚拟机内存使用状况。</p><pre><code># free 代表系统当前还剩余的内存（单位MB）。
              total        used        free      shared  buff/cache   available
Mem:           1838         350        1113
</code></pre><p>如果内存剩余量不足100M，可能就会导致上面的问题。</p><p>解决内存不足的问题可以：
（1）关闭虚拟机，右键点击镜像，选择<code>设置</code>，调整内存大小。
<img src=/static/img/spark/Snipaste_2020-04-19_23-11-31.png alt></p><p>（2）关闭不使用的服务，例如 yarn，hive 都可以退出。</p><p>（3）如果你的电脑内存资源很紧张，可以考虑开启 swap 分区，相当于使用硬盘空间作为虚拟内存。虚拟机<code>free -m</code>命令显示 swap 的total 为0，则表示swap分区没有开启，开启命令如下：</p><ul><li>创建 swap 文件</li></ul><pre><code>#这里count的数值就是 swap 分区的大小，单位为MB
sudo dd if=/dev/zero of=/swapfile count=2048 bs=1M
</code></pre><ul><li>激活 swapfile</li></ul><pre><code>sudo chmod 600 /swapfile
sudo mkswap /swapfile

#如果运行成功会显示
#Setting up swapspace version 1, size = 2097148 KiB
#no label, UUID=...
</code></pre><ul><li>开启 swap</li></ul><pre><code>sudo swapon /swapfile

#查看 swap 的 total 值是否是你配置的大小
free -m
</code></pre><ul><li>添加 Swap 自动开启的配置。</li></ul><pre><code>sudo vim /etc/fstab

#文件最后一行加上
/swapfile none swap sw 0 0
</code></pre><h3 id=问题2提示连接不上-hdfscall-from-node0127001-to-node08020-failed-on-connection-exception>问题2：提示连接不上 HDFS，<code>Call From node0/127.0.0.1 to node0:8020 failed on connection exception</code>。</h3><p>答：可能是由于node0没有配置对应的IP，导致连接不上HDFS。请保证 hosts 的配置包含以下2行。<code>sudo vim /etc/hosts</code></p><pre><code>127.0.0.1  node0
127.0.0.1  localhost
</code></pre><h3 id=问题3提示the-specified-datastore-driver-commysqljdbcdriver-was-not-found-in-the-classpath>问题3：提示“The specified datastore driver (&ldquo;com.mysql.jdbc.Driver&rdquo;) was not found in the CLASSPATH.”</h3><p>答：这是由于 PySpark 没有找到 MySQL 驱动，导致无法连接上 Hive 的 MySQL。请使用以下命令启动PySpark。</p><pre><code>pyspark --master spark://node0:7077 --jars /opt/hive/lib/mysql-connector-java-5.1.48.jar
</code></pre><h3 id=问题4提示o25sqluable-to-instantiate-orgapachehadoophiveqlmetadatasessionhivemetastoreclient>问题4：提示“o25.sql:&mldr;Uable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;”</h3><p>答：这个问题可能原因：</p><ul><li><p>（1）hosts配置不正确导致无法连接 MySQL。请参考问题2解决方法。</p></li><li><p>（2）PySpark 没有找到 MySQL 的 JDBC 驱动。请参考问题3解决方法。</p></li></ul></div><div class=my-4><a href=/tags/spark/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#spark</a>
<a href=/tags/sql/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#SQL</a>
<a href=/tags/sparksql/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#SparkSQL</a></div><div class=py-2><div class="flex flex-col md:flex-row items-center my-8"><a href=/authors/heis/ class="w-24 h-24 md:mr-4"><img src=/static/img/authors/heis.png class="w-full bg-primary-bg rounded-full" alt=Avatar></a><div class="w-full md:w-auto mt-4 md:mt-0"><a href=/authors/heis/ class="block font-bold text-lg pb-1 mb-2 border-b">黄老师</a>
<span class="block pb-2"></span><a href=mailto:heishuangzy@qq.com class=mr-1><i class="fas fa-envelope"></i></a><a href=https://gitee.com/heis/ class=mr-1><i class="fab fa-git"></i></a></div></div></div><div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t"><div id=presec><span class="block font-bold">上一页</span>
<a href=/docs/spark-exp/spark-exp04/ class=block>第四章 Spark RDD 实验手册</a></div><div id=nextsec class="md:text-right mt-4 md:mt-0"><span class="block font-bold">下一页</span>
<a href=/docs/spark-exp/spark-exp06/ class=block>第六章 Spark Streaming 实验手册</a></div></div></div><div class="hidden lg:block lg:w-1/4"><div class="sticky top-16 z-10 hidden lg:block px-6 py-4 bg-secondary-bg pt-16 -mt-16"><span class="text-lg font-semibold">本页内容</span></div><div class="sticky-toc hidden lg:block px-6 pb-6 pt-10 -mt-10 border-l"><nav id=TableOfContents><ul><li><a href=#版本>【版本】</a></li><li><a href=#实验名称-实验51rdd-dataframe-api-练习>【实验名称】 实验5.1：RDD DataFrame API 练习</a><ul><li><a href=#实验目的>【实验目的】</a></li><li><a href=#实验原理>【实验原理】</a></li><li><a href=#实验环境>【实验环境】</a></li><li><a href=#实验要求>【实验要求】</a></li></ul></li><li><a href=#实验名称-实验52自行设计实现rdd转为dataframe>【实验名称】 实验5.2：自行设计实现RDD转为DataFrame</a><ul><li><a href=#实验目的-1>【实验目的】</a></li><li><a href=#实验原理-1>【实验原理】</a></li><li><a href=#实验环境-1>【实验环境】</a></li><li><a href=#实验资源>【实验资源】</a></li><li><a href=#实验背景>【实验背景】</a></li><li><a href=#实验要求-1>【实验要求】</a></li></ul></li><li><a href=#实验名称-实验53部署-spark-访问-hive-数据>【实验名称】 实验5.3：部署 Spark 访问 Hive 数据</a><ul><li><a href=#实验目的-2>【实验目的】</a></li><li><a href=#实验环境-2>【实验环境】</a></li><li><a href=#实验资源-1>【实验资源】</a></li><li><a href=#实验原理-2>【实验原理】</a></li><li><a href=#hive优化参数配置和解决常见问题>【Hive优化参数配置和解决常见问题】</a></li><li><a href=#实验步骤>【实验步骤】</a></li></ul></li><li><a href=#实验名称-实验54使用dataframe分析出租车的-gps-信息>【实验名称】 实验5.4：使用DataFrame分析出租车的 GPS 信息</a><ul><li><a href=#实验目的-3>【实验目的】</a></li><li><a href=#实验环境-3>【实验环境】</a></li><li><a href=#实验原理-3>【实验原理】</a></li><li><a href=#实验资源-2>【实验资源】</a></li><li><a href=#实验要求-2>【实验要求】</a></li></ul></li><li><a href=#实验名称-实验55使用sparksql分析图书信息>【实验名称】 实验5.5：使用SparkSQL分析图书信息</a><ul><li><a href=#实验目的-4>【实验目的】</a></li><li><a href=#实验原理-4>【实验原理】</a></li><li><a href=#实验环境-4>【实验环境】</a></li><li><a href=#实验要求-3>【实验要求】</a></li></ul></li><li><a href=#实验名称-实验56dataframe的保存和加载>【实验名称】 实验5.6：DataFrame的保存和加载</a><ul><li><a href=#实验目的-5>【实验目的】</a></li><li><a href=#实验原理-5>【实验原理】</a></li><li><a href=#实验环境-5>【实验环境】</a></li><li><a href=#实验步骤-1>【实验步骤】</a></li></ul></li><li><a href=#常见问题解答faq>【常见问题解答FAQ】</a><ul><li><a href=#问题1使用-spark-sql-的时候频繁遇到-py4j-的connection-error-或者-jvm-提示cannot-allocate-memory错误>问题1：使用 Spark SQL 的时候频繁遇到 Py4j 的<code>Connection error</code> 或者 JVM 提示<code>Cannot allocate memory</code>错误。</a></li><li><a href=#问题2提示连接不上-hdfscall-from-node0127001-to-node08020-failed-on-connection-exception>问题2：提示连接不上 HDFS，<code>Call From node0/127.0.0.1 to node0:8020 failed on connection exception</code>。</a></li><li><a href=#问题3提示the-specified-datastore-driver-commysqljdbcdriver-was-not-found-in-the-classpath>问题3：提示“The specified datastore driver (&ldquo;com.mysql.jdbc.Driver&rdquo;) was not found in the CLASSPATH.”</a></li><li><a href=#问题4提示o25sqluable-to-instantiate-orgapachehadoophiveqlmetadatasessionhivemetastoreclient>问题4：提示“o25.sql:&mldr;Uable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;”</a></li></ul></li></ul></nav></div><script>window.addEventListener('DOMContentLoaded',()=>{enableStickyToc();});</script></div></div></div></div></div><div id=outerdiv><div id=innerdiv><img id=bigimg src></div></div><script>document.addEventListener('DOMContentLoaded',()=>{hljs.highlightAll();hljs.initLineNumbersOnLoad({singleLine:true});changeSidebarHeight();switchDocToc();$("img").css("cursor:pointer")
$("img").click(function(){var _this=$(this);imgShow("#outerdiv","#innerdiv","#bigimg",_this);});$("ol").each(function(index){if(/\d{1,4}/.test($(this).attr("start"))){let idx=parseInt($(this).attr("start"));let lis=$(this).find("li");if(lis.length>0){lis.each(function(index){$(this).attr("id","step"+(idx+index));});}else{$(this).attr("id","step"+idx);}}});});</script><script src=/static/js/clipboard.min.js></script><script src=/static/js/codecopy.bundle.js?968></script></div></div></main><footer class=pl-scrollbar><div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2021 <a href=https://www.wangchucheng.com/>C. Wang</a> and <a href=https://www.ruiqima.com/>R. Ma</a>
&#183; Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>
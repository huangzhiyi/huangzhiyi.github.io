<!doctype html><html lang=zh><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Hadoop Part 2 - 部署 Hadoop 完全分布模式 | Hadoop 课程资源汇总 | Heis</title><meta name=generator content="Hugo Eureka 0.8.4"><link rel=stylesheet href=/css/eureka.min.css><script defer src=/js/eureka.min.js></script><script src=/static/js/jquery-3.6.0.min.js></script><script src=/static/js/heis.js></script><link rel=stylesheet href=/static/css/heis.css media=print onload="this.media='all';this.onload=null" crossorigin><link rel=stylesheet href=/static/css/highlight-css/solarized-light.min.css media=print onload="this.media='all';this.onload=null" crossorigin><script defer src=/static/js/highlight.min.js crossorigin></script><script defer src=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js integrity="sha256-uNYoXefWRqv+PsIF/OflNmwtKM4lStn9yrz2gVl6ymo=" crossorigin></script><script defer src=/static/js/highlightjs-line-numbers.min.js></script><link rel=icon type=image/png sizes=32x32 href=/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_2.png><link rel=apple-touch-icon sizes=180x180 href=/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_2.png><meta name=description content="Hadoop Part 2 - 部署 Hadoop 完全分布模式"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"课程文档","item":"/docs/"},{"@type":"ListItem","position":2,"name":"Hadoop 课程资源汇总","item":"/docs/hadoop-c/"},{"@type":"ListItem","position":3,"name":"Hadoop Part 2 - 部署 Hadoop 完全分布模式","item":"/docs/hadoop-c/hadoop-c02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"/docs/hadoop-c/hadoop-c02/"},"headline":"Hadoop Part 2 - 部署 Hadoop 完全分布模式 | Hadoop 课程资源汇总 | Heis","datePublished":"2021-03-19T00:44:00+08:00","dateModified":"2021-03-19T00:44:00+08:00","wordCount":2111,"author":{"@type":"Person","name":"heis"},"publisher":{"@type":"Person","name":"C. Wang","logo":{"@type":"ImageObject","url":"/images/icon.png"}},"description":"Hadoop Part 2 - 部署 Hadoop 完全分布模式"}</script><meta property="og:title" content="Hadoop Part 2 - 部署 Hadoop 完全分布模式 | Hadoop 课程资源汇总 | Heis"><meta property="og:type" content="article"><meta property="og:image" content="/images/icon.png"><meta property="og:url" content="/docs/hadoop-c/hadoop-c02/"><meta property="og:description" content="Hadoop Part 2 - 部署 Hadoop 完全分布模式"><meta property="og:locale" content="zh"><meta property="og:site_name" content="Heis"><meta property="article:published_time" content="2021-03-19T00:44:00+08:00"><meta property="article:modified_time" content="2021-03-19T00:44:00+08:00"><meta property="article:section" content="docs"><meta property="article:tag" content="hadoop"><meta property="article:tag" content="大数据"><meta property="og:see_also" content="/docs/hadoop-c/hadoop-c01/"><script>var _hmt=_hmt||[];(function(){var hm=document.createElement("script");hm.src="https://hm.baidu.com/hm.js?f58b6a2675cf52000232edb4d109eccc";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script><body class="flex flex-col min-h-screen"><header class="fixed flex items-center w-full min-h-16 pl-scrollbar z-50 bg-secondary-bg shadow-sm"><div class="w-full max-w-screen-xl mx-auto"><script>let storageColorScheme=localStorage.getItem("lightDarkMode")
if(((storageColorScheme=='Auto'||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches)||storageColorScheme=="Dark"){document.getElementsByTagName('html')[0].classList.add('dark')}</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="mr-6 text-primary-text text-xl font-bold">Heis</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent mr-4">文章</a>
<a href=/docs/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item mr-4">课程文档</a>
<a href=/static/dm.html class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent mr-4">点名</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>浅色</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>深色</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>自动</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById('lightDarkMode')
if(storageColorScheme==null||storageColorScheme=='Auto'){document.addEventListener('DOMContentLoaded',()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change',switchDarkMode)})}else if(storageColorScheme=="Light"){element.firstElementChild.classList.remove('fa-adjust')
element.firstElementChild.setAttribute("data-icon",'sun')
element.firstElementChild.classList.add('fa-sun')}else if(storageColorScheme=="Dark"){element.firstElementChild.classList.remove('fa-adjust')
element.firstElementChild.setAttribute("data-icon",'moon')
element.firstElementChild.classList.add('fa-moon')}
document.addEventListener('DOMContentLoaded',()=>{getcolorscheme();switchBurger();});</script></div></header><main class="flex-grow pt-16"><div class=pl-scrollbar><div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto"><div class=lg:pt-12><div class="flex flex-col md:flex-row bg-secondary-bg rounded"><div class="md:w-1/4 lg:w-1/5 border-r"><div class="sticky top-16 pt-6"><div id=sidebar-title class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text"><span class=font-semibold>目录</span>
<i class="fas fa-caret-right ml-1"></i></div><div id=sidebar-toc class="hidden md:block overflow-y-auto mx-6 md:mx-0 pr-6 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent"><div class="flex flex-wrap ml-4 -mr-2 p-2 bg-secondary-bg md:bg-primary-bg rounded"><a class=hover:text-eureka href=/docs/hadoop-c/>Hadoop 课程资源汇总</a></div><ul class=pl-6><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c01/>Hadoop Part 1 - 模板机制作</a></div></li><li class=py-2><div><a class="text-eureka hover:text-eureka" href=/docs/hadoop-c/hadoop-c02/>Hadoop Part 2 - 部署 Hadoop 完全分布模式</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c03/>Hadoop Part 3 - 通过Shell命令访问HDFS</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c04/>Hadoop Part 4 - 搭建 Hadoop 开发环境</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c05/>Hadoop Part 5 - HDFS Java 编程访问</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c06/>Hadoop Part 6 - 编写 MapReduce 程序</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c07/>Hadoop Part 7 - 部署 Hive 和 Hive 常用操作</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c08/>Hadoop Part 8 - 部署 HBase 和 HBase 常用操作(选做)</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c09/>Hadoop Part 9 - 部署 ZooKeeper 和 ZooKeeper 常用操作(选做)</a></div></li><li class=py-2><div><a class=hover:text-eureka href=/docs/hadoop-c/hadoop-c10/>Hadoop Part 10 - Flume 和 Sqoop 操作实例（选做）</a></div></li></ul></div></div></div><div class="w-full md:w-3/4 lg:w-4/5 pb-8 pt-2 md:pt-8"><div class="w-full lg:w-3/4 pl-6 ml-0 mr-auto"><h1 class="font-bold text-3xl text-primary-text">Hadoop Part 2 - 部署 Hadoop 完全分布模式</h1><div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text"><div class="mr-6 my-2"><i class="fas fa-calendar mr-1"></i><span>2021-03-19</span></div><div class="mr-6 my-2"><i class="fas fa-clock mr-1"></i><span>5分钟阅读时长</span></div><div class="mr-6 my-2"><i class="fas fa-folder mr-1"></i><a href=/categories/hadoop/ class=hover:text-eureka>hadoop</a></div></div></div><div class=flex><div class="w-full lg:w-3/4 px-6"><div class=content><h2 id=版本>【版本】</h2><p>当前版本号<code>v20210415</code></p><div class=tbl-start></div><table><thead><tr><th>版本</th><th>修改说明</th></tr></thead><tbody><tr><td>v20210415</td><td>修改了虚拟机名称的描述避免误解为hostname</td></tr><tr><td>v20210414</td><td>修改了步骤1，提示要使用hadoop用户进行登录</td></tr><tr><td>v20210408</td><td>新增配置到core-site.xml，修正jobhistoryserver意外退出的问题</td></tr><tr><td>v20210407-1</td><td>修正NodeB和NodeC的公钥操作</td></tr><tr><td>v20210407</td><td>修正authorized_keys 在 NodeB 和 NodeC 的操作</td></tr><tr><td>v20210406</td><td>修改了mapred-site.xml出现的配置错误；新增了把key写入 authorized_keys 步骤；增加了slaves的说明，避免出错；</td></tr><tr><td>v20210319</td><td>初始化版本</td></tr></tbody></table><div class=tbl-end style=height:10px></div><h2 id=hadoop-part-2---部署-hadoop-完全分布模式>Hadoop Part 2 - 部署 Hadoop 完全分布模式</h2><h3 id=实验目的>【实验目的】</h3><ul><li>掌握搭建 Hadoop 完全分布模式</li><li>熟练掌握Linux命令（vi、tar、mv等等）的使用</li><li>掌握VMWare、XShell等客户端的使用</li></ul><h3 id=实验环境>【实验环境】</h3><ul><li>内存：至少4G</li><li>硬盘：至少空余40G</li><li>操作系统: 64位 Windows系统。</li></ul><h3 id=实验资源>【实验资源】</h3><ul><li>XShell</li><li>CentOS 7.4系统镜像</li><li>VMWare WorkStation Pro</li><li>Hadoop 安装包</li></ul><pre><code>下载链接：https://pan.baidu.com/s/1ghde86wcK6pwg1fdSSWg0w
提取码：v3wv
</code></pre><h3 id=虚拟机操作实验步骤>【虚拟机操作实验步骤】</h3><ol><li>关闭 Part1 完成的 HadoopTmpl 模板机。依次克隆出3台虚拟机，名称，主机名和 IP 地址如下表所示，注意替换为你的学号后4位。</li></ol><div class=tbl-start></div><table><thead><tr><th>虚拟机名称</th><th>hostname</th><th>IP地址</th></tr></thead><tbody><tr><td>节点A主机（Namenode）</td><td>nodea+你学号后4位</td><td>10.0.0.71</td></tr><tr><td>节点B主机（Datanode）</td><td>nodeb+你学号后4位</td><td>10.0.0.72</td></tr><tr><td>节点C主机（Datanode）</td><td>nodec+你学号后4位</td><td>10.0.0.73</td></tr></tbody></table><div class=tbl-end style=height:10px></div><p><img src=/static/img/hadoop-c02/Snipaste_2021-03-19_00-49-54.png alt>
<img src=/static/img/hadoop-c02/Snipaste_2021-02-28_23-25-20.png alt>
<img src=/static/img/hadoop-c02/Snipaste_2021-02-28_23-25-45.png alt>
<img src=/static/img/hadoop-c02/Snipaste_2021-03-19_01-04-20.png alt></p><ol start=2><li>依次启动克隆的虚拟机，修改为对应的 hostname 和 IP。</li></ol><pre><code>vim /etc/hostname
</code></pre><pre><code>vim /etc/sysconfig/network-scripts/ifcfg-eth0
</code></pre><ol start=3><li>重启克隆的3台虚拟机，配置 XShell 连接虚拟机，使用<code>hadoop</code>用户登录，密码为<code>123456</code>。</li></ol><h3 id=nodea节点实验步骤>【NodeA节点实验步骤】</h3><ol><li>使用 Hadoop 用户登录 NodeA 节点。如果使用root登录的可以使用以下命令切换到hadoop用户。</li></ol><pre><code>su hadoop
</code></pre><ol start=2><li>配置免密登录。首先生成密钥对，运行以下命令，直接回车（Enter）3次。</li></ol><pre><code>ssh-keygen -t rsa
</code></pre><ol start=3><li>查看目录下是否有公钥<code>id_rsa.pub</code>和私钥<code>id_rsa</code>。</li></ol><pre><code>cd ~/.ssh
ls
</code></pre><ol start=4><li>执行以下命令，把公钥写入本机授权文件。</li></ol><pre><code>cat id_rsa.pub &gt;&gt; authorized_keys
</code></pre><ol start=5><li>查看授权文件内的公钥内容。</li></ol><pre><code>cd ~/.ssh
cat authorized_keys
</code></pre><ol start=6><li>启动<code>NodeB</code>和<code>NodeC</code>2个节点。在<code>NodaA</code>上面运行以下命令，把公钥拷贝到<code>NodeB</code>和<code>NodeC</code>。</li></ol><pre><code>ssh-copy-id -i ~/.ssh/id_rsa.pub nodeb+你学号后4位 -f
ssh-copy-id -i ~/.ssh/id_rsa.pub nodec+你学号后4位 -f
</code></pre><pre><code>#系统询问是否连接，输入yes
Are you sure you want to continue connecting (yes/no)? yes
#输入 hadoop 登录密码
hadoop@nodeb9999's password:
</code></pre><ol start=7><li>在<code>NodeB</code>和<code>NodeC</code>2个节点分别执行以下命令，查看是否包含来自<code>NodeA</code>的公钥。</li></ol><pre><code>cat  authorized_keys
</code></pre><ol start=8><li>备份和编辑 Hadoop 的 core-site.xml 配置文件。在configuration 标签内添加配置，注意替换为你的学号后4位。</li></ol><pre><code>cp /opt/hadoop/etc/hadoop/core-site.xml{,.bak}
vim /opt/hadoop/etc/hadoop/core-site.xml
</code></pre><pre><code>&lt;configuration&gt;
  &lt;!-- HDFS 访问地址 --&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://nodea+你学号后4位:8020&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/opt/hadoop/tmp&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;fs.trash.interval&lt;/name&gt;
    &lt;value&gt;1440&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;
    &lt;value&gt;hadoop&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><ol start=9><li>备份和编辑 Hadoop 的 hdfs-site.xml 配置文件。请注意替换为你的学号。</li></ol><pre><code>cp /opt/hadoop/etc/hadoop/hdfs-site.xml{,.bak}
vim /opt/hadoop/etc/hadoop/hdfs-site.xml
</code></pre><pre><code>&lt;configuration&gt;
  &lt;!-- secondary namenode 访问地址--&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.secondary.http.address&lt;/name&gt;
    &lt;value&gt;nodea+你学号后4位:50090&lt;/value&gt;
  &lt;/property&gt;
  &lt;!-- HDFS 副本数量 --&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><ol start=10><li>新建一个 masters 配置文件，写入 Secondary NameNode 的主机名。</li></ol><pre><code>vim /opt/hadoop/etc/hadoop/masters
</code></pre><p>写入以下内容，注意替换为你的学号后4位。</p><pre><code>nodea+你的学号后4位
</code></pre><ol start=11><li>备份和编辑 Hadoop 的 mapred-site.xml 配置文件。注意替换为你的学号后4位。</li></ol><pre><code>cp /opt/hadoop/etc/hadoop/mapred-site.xml.template /opt/hadoop/etc/hadoop/mapred-site.xml
vim /opt/hadoop/etc/hadoop/mapred-site.xml
</code></pre><pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
    &lt;value&gt;nodea+你学号后4位:10020&lt;/value&gt;
    &lt;description&gt;Host and port for Job History Server (default 0.0.0.0:10020)&lt;/description&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><ol start=12><li>备份和编辑 Hadoop 的 yarn-site.xml 配置文件。注意替换为你的学号后4位。</li></ol><pre><code>cp /opt/hadoop/etc/hadoop/yarn-site.xml{,.bak}
vim /opt/hadoop/etc/hadoop/yarn-site.xml
</code></pre><pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    &lt;value&gt;nodea+你学号后4位&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><ol start=13><li>编辑 slaves ，清除原来的所有内容，增加配置 DataNode 节点信息。注意替换为你的学号后4位。</li></ol><pre><code>vim /opt/hadoop/etc/hadoop/slaves
</code></pre><pre><code>nodeb+你学号后4位
nodec+你学号后4位
</code></pre><ol start=14><li>修改 hadoop-env.sh，在第1行加入以下代码。</li></ol><pre><code>vim /opt/hadoop/etc/hadoop/hadoop-env.sh
</code></pre><pre><code>JAVA_HOME=/opt/jdk8
</code></pre><ol start=15><li>把<code>NodeA</code>节点的 Hadoop /opt/hadoop/etc/hadoop 下所有配置文件发送到<code>NodeB</code>和<code>NodeC</code>。如果上面的配置文件有修改，也需要同步发送到<code>NodeB</code>和<code>NodeC</code>节点。</li></ol><pre><code>cd /opt/hadoop/etc/
scp -r hadoop hadoop@nodeb+你学号后4位:/opt/hadoop/etc/
scp -r hadoop hadoop@nodec+你学号后4位:/opt/hadoop/etc/
</code></pre><ol start=16><li>格式化 HDFS。请勿重复执行，因为会导致 datanode 和 namenode 的集群ID不一致，造成HDFS出错。</li></ol><pre><code>hdfs namenode -format
</code></pre><ol start=17><li>新建启动和停止 Hadoop 的脚本。</li></ol><ul><li>创建启动脚本，注意替换为你的学号后4位。</li></ul><pre><code>vim /opt/hadoop/sbin/start-hdp.sh
</code></pre><pre><code>#!/usr/bin/env bash
echo &quot;Start Hadoop by 你的学号后4位&quot;
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon.sh start historyserver
</code></pre><ul><li>创建停止脚本，注意替换为你的学号后4位。</li></ul><pre><code>vim /opt/hadoop/sbin/stop-hdp.sh
</code></pre><pre><code>#!/usr/bin/env bash
echo &quot;Stop Hadoop by 你的学号后4位&quot;
mr-jobhistory-daemon.sh stop historyserver
stop-yarn.sh
stop-dfs.sh
</code></pre><ul><li>重启脚本</li></ul><pre><code>vim /opt/hadoop/sbin/restart-hdp.sh
</code></pre><pre><code>#!/usr/bin/env bash
stop-hdp.sh
start-hdp.sh
</code></pre><ol start=18><li>修改创建的脚本的权限。</li></ol><pre><code>cd /opt/hadoop/sbin/
chmod 744 start-hdp.sh stop-hdp.sh restart-hdp.sh
</code></pre><ol start=19><li>使用脚本启动 Hadoop。</li></ol><pre><code>start-hdp.sh
</code></pre><h3 id=实验验证步骤>【实验验证步骤】</h3><ol><li>在<code>NodeA</code>输入<code>jps</code>命令，观察是否有以下进程。</li></ol><pre><code>NameNode
Jps
ResourceManager
SecondaryNameNode
JobHistoryServer
</code></pre><ol start=2><li>在<code>NodeB</code>和<code>NodeC</code>分别输入<code>jps</code>命令，观察是否有以下进程。</li></ol><pre><code>DataNode
NodeManager
Jps
</code></pre><ol start=3><li><p>打开宿主机浏览器，访问 HDFS Web界面 <a href=http://10.0.0.71:50070/ target=_blank>http://10.0.0.71:50070/</a>
<img src=/static/img/hadoop-c02/Snipaste_2021-03-19_23-07-26.png alt></p></li><li><p>上传<code>countryroad.txt</code>到<code>NodeA</code>的<code>/home/hadoop</code></p></li><li><p>把<code>countryroad.txt</code>从 CentOS 文件系统上传到 HDFS 文件系统。</p></li></ol><pre><code>hdfs dfs -mkdir /part2
hdfs dfs -put /home/hadoop/countryroad.txt /part2
hdfs dfs -ls /part2
</code></pre><ol start=6><li>运行 Hadoop 自带的 Wordcount 程序，观察输出的内容。</li></ol><pre><code>cd $HADOOP_HOME/share/hadoop/mapreduce
hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /part2/countryroad.txt /output
</code></pre><ol start=7><li>程序执行过程中，可以访问 Yarn Web 界面查看任务进展。 <a href=http://10.0.0.71:8088/cluster/apps target=_blank>http://10.0.0.71:8088/cluster/apps</a></li></ol><p><img src=/static/img/hadoop-c02/Snipaste_2021-03-19_23-41-58.png alt></p><ol start=8><li>等待程序运行完毕，观察输出的内容</li></ol><pre><code>hdfs dfs -cat /output/part-r-00000
</code></pre></div><div class=my-4><a href=/tags/hadoop/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#hadoop</a>
<a href=/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#大数据</a></div><div class=py-2><div class="flex flex-col md:flex-row items-center my-8"><a href=/authors/heis/ class="w-24 h-24 md:mr-4"><img src=/static/img/authors/heis.png class="w-full bg-primary-bg rounded-full" alt=Avatar></a><div class="w-full md:w-auto mt-4 md:mt-0"><a href=/authors/heis/ class="block font-bold text-lg pb-1 mb-2 border-b">黄老师</a>
<span class="block pb-2"></span><a href=mailto:heishuangzy@qq.com class=mr-1><i class="fas fa-envelope"></i></a><a href=https://gitee.com/heis/ class=mr-1><i class="fab fa-git"></i></a></div></div></div><div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t"><div><span class="block font-bold">上一页</span>
<a href=/docs/hadoop-c/hadoop-c01/ class=block>Hadoop Part 1 - 模板机制作</a></div><div class="md:text-right mt-4 md:mt-0"><span class="block font-bold">下一页</span>
<a href=/docs/hadoop-c/hadoop-c03/ class=block>Hadoop Part 3 - 通过Shell命令访问HDFS</a></div></div></div><div class="hidden lg:block lg:w-1/4"><div class="sticky top-16 z-10 hidden lg:block px-6 py-4 bg-secondary-bg pt-16 -mt-16"><span class="text-lg font-semibold">本页内容</span></div><div class="sticky-toc hidden lg:block px-6 pb-6 pt-10 -mt-10 border-l"><nav id=TableOfContents><ul><li><a href=#版本>【版本】</a></li><li><a href=#hadoop-part-2---部署-hadoop-完全分布模式>Hadoop Part 2 - 部署 Hadoop 完全分布模式</a><ul><li><a href=#实验目的>【实验目的】</a></li><li><a href=#实验环境>【实验环境】</a></li><li><a href=#实验资源>【实验资源】</a></li><li><a href=#虚拟机操作实验步骤>【虚拟机操作实验步骤】</a></li><li><a href=#nodea节点实验步骤>【NodeA节点实验步骤】</a></li><li><a href=#实验验证步骤>【实验验证步骤】</a></li></ul></li></ul></nav></div><script>window.addEventListener('DOMContentLoaded',()=>{enableStickyToc();});</script></div></div></div></div></div><div id=outerdiv><div id=innerdiv><img id=bigimg src></div></div><script>document.addEventListener('DOMContentLoaded',()=>{hljs.highlightAll();hljs.initLineNumbersOnLoad({singleLine:true});changeSidebarHeight();switchDocToc();$("img").css("cursor:pointer")
$("img").click(function(){var _this=$(this);imgShow("#outerdiv","#innerdiv","#bigimg",_this);});})</script><script src=/static/js/clipboard.min.js></script><script src=/static/js/codecopy.bundle.js?658></script></div></div></main><footer class=pl-scrollbar><div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2021 <a href=https://www.wangchucheng.com/>C. Wang</a> and <a href=https://www.ruiqima.com/>R. Ma</a>
&#183; Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>